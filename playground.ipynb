{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python -m venv mamba_env\n",
    "pip install packaging wheel torch ipykernel datasets\n",
    "pip install accelerate -U\n",
    "pip install .\n",
    "python -m ipykernel install --user --name=mamba_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, GPTNeoXForCausalLM, GPTNeoXModel\n",
    "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
    "import sys\n",
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "\n",
    "# Take in the model you want to train\n",
    "model_name = \"state-spaces/mamba-130m\"\n",
    "\n",
    "# Choose a tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "tokenizer.eos_token = \"<|endoftext|>\"\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer, GPTNeoXModel\n",
    "\n",
    "model = GPTNeoXForCausalLM.from_pretrained(\n",
    "  \"EleutherAI/pythia-160m-deduped\",\n",
    "  #output_hidden_states=True,\n",
    "  #revision=\"step3000\",\n",
    "  #cache_dir=\"./pythia-70m-deduped/step3000\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MambaLMHeadModel.from_pretrained(\n",
    "    model_name, \n",
    "    device=\"cuda\", \n",
    "    dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a Trivia QA bot.\n",
      "Answer the following question succinctly and accurately.\n",
      "\n",
      "Q: What is the capital of France?\n",
      "A: Paris\n",
      "\n",
      "Q: Who invented the segway?\n",
      "A: Dean Kamen\n",
      "\n",
      "Q: What is the fastest animal?\n",
      "A: Cheetah\n",
      "\n",
      "Q: Give me three steps to improve my diet, and include some evidence\n",
      "A:\n",
      "You are a Trivia QA bot.\n",
      "Answer the following question succinctly and accurately.\n",
      "\n",
      "Q: What is the capital of France?\n",
      "A: Paris\n",
      "\n",
      "Q: Who invented the segway?\n",
      "A: Dean Kamen\n",
      "\n",
      "Q: What is the fastest animal?\n",
      "A: Cheetah\n",
      "\n",
      "Q: Give me three steps to improve my diet, and include some evidence\n",
      "A:\n",
      "tensor([[ 1394,   403,   247,   308,  1069,   571,  1165,    34, 17994,    15,\n",
      "           187, 32869,   253,  1563,  1953, 18382,  4291,   314,   285, 13613,\n",
      "            15,   187,   187,    50,    27,  1737,   310,   253,  5347,   273,\n",
      "          6181,    32,   187,    34,    27,  7785,   187,   187,    50,    27,\n",
      "          8452, 23179,   253,  8753,  1106,    32,   187,    34,    27, 19172,\n",
      "           611, 18986,   187,   187,    50,    27,  1737,   310,   253, 22583,\n",
      "          5893,    32,   187,    34,    27,  4661,   292,  1240,   187,   187,\n",
      "            50,    27,  7918,   479,  1264,  5018,   281,  3157,   619,  6196,\n",
      "            13,   285,  2486,   690,  1941,   187,    34,    27]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Take the user input from the command line\n",
    "user_message = \"Give me three steps to improve my diet, and include some evidence\"#input(\"\\n> \")\n",
    "\n",
    "# Create a prompt\n",
    "n_shot_prompting = [\n",
    "    {\n",
    "        \"question\": \"What is the capital of France?\",\n",
    "        \"answer\": \"Paris\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Who invented the segway?\",\n",
    "        \"answer\": \"Dean Kamen\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the fastest animal?\",\n",
    "        \"answer\": \"Cheetah\"\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt = f\"You are a Trivia QA bot.\\nAnswer the following question succinctly and accurately.\"\n",
    "prompt = f\"{prompt}\\n\\n\" + \"\\n\\n\".join([f\"Q: {p['question']}\\nA: {p['answer']}\" for p in n_shot_prompting])\n",
    "prompt = f\"{prompt}\\n\\nQ: {user_message}\\nA:\"\n",
    "\n",
    "# Debug print to make sure our prompt looks good\n",
    "print(prompt)\n",
    "\n",
    "# Encode the text to token IDs\n",
    "input_ids = torch.LongTensor([tokenizer.encode(prompt)]).cuda()\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "# Encode the prompt into integers and convert to a tensor on the GPU\n",
    "input_ids = torch.LongTensor([tokenizer.encode(prompt)]).cuda()\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Generate an output sequence of tokens given the input\n",
    "# \"out\" will contain the raw token ids as integers\n",
    "out = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=256,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(\n",
    "    input_ids=input_ids,\n",
    "    output_hidden_states=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([1, 88, 768])\n",
      "1 torch.Size([1, 88, 768])\n",
      "2 torch.Size([1, 88, 768])\n",
      "3 torch.Size([1, 88, 768])\n",
      "4 torch.Size([1, 88, 768])\n",
      "5 torch.Size([1, 88, 768])\n",
      "6 torch.Size([1, 88, 768])\n",
      "7 torch.Size([1, 88, 768])\n",
      "8 torch.Size([1, 88, 768])\n",
      "9 torch.Size([1, 88, 768])\n",
      "10 torch.Size([1, 88, 768])\n",
      "11 torch.Size([1, 88, 768])\n",
      "12 torch.Size([1, 88, 768])\n"
     ]
    }
   ],
   "source": [
    "for k,X in enumerate(out.hidden_states):\n",
    "    print(k, X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "You are a Trivia QA bot.\n",
      "Answer the following question succinctly and accurately.\n",
      "\n",
      "Q: What is the capital of France?\n",
      "A: Paris\n",
      "\n",
      "Q: Who invented the segway?\n",
      "A: Dean Kamen\n",
      "\n",
      "Q: What is the fastest animal?\n",
      "A: Cheetah\n",
      "\n",
      "Q: Give me three steps to improve my diet, and include some evidence\n",
      "A: I'm not sure.\n",
      "\n",
      "Q: What is the most popular game in the world?\n",
      "A: Minecraft\n",
      "\n",
      "Q: What is the most popular game in the world?\n",
      "A: Minecraft\n",
      "\n",
      "Q: What is the most popular game in the world?\n",
      "A: Minecraft\n",
      "\n",
      "Q: What is the most popular game in the world?\n",
      "A: Minecraft\n",
      "\n",
      "Q: What is the most popular game in the world?\n",
      "A: Minecraft\n",
      "\n",
      "Q: What is the most popular game in the world?\n",
      "A: Minecraft\n",
      "\n",
      "Q: What is the most popular game in the world?\n",
      "A: Minecraft\n",
      "\n",
      "Q: What is the most popular game in the world?\n",
      "A: Minecraft\n",
      "\n",
      "Q: What is the most popular game in\n",
      " I'm not sure.\n",
      "\n",
      "Q: What is the most popular game in the world?\n",
      "A: Minecraft\n",
      "\n",
      "Q: What is the most popular game in the world?\n",
      "A: Minecraft\n",
      "\n",
      "Q: What is the most popular game in the world?\n",
      "A: Minecraft\n",
      "\n",
      "Q: What is the most popular game in the world?\n",
      "A: Minecraft\n",
      "\n",
      "Q: What is the most popular game in the world?\n",
      "A: Minecraft\n",
      "\n",
      "Q: What is the most popular game in the world?\n",
      "A: Minecraft\n",
      "\n",
      "Q: What is the most popular game in the world?\n",
      "A: Minecraft\n",
      "\n",
      "Q: What is the most popular game in the world?\n",
      "A: Minecraft\n",
      "\n",
      "Q: What is the most popular game in\n"
     ]
    }
   ],
   "source": [
    "# you must use the tokenizer to decode them back into strings\n",
    "decoded = tokenizer.batch_decode(out)[0]\n",
    "print(\"=\"*80)\n",
    "print(decoded)\n",
    "# out returns the whole sequence plus the original\n",
    "cleaned = decoded.replace(prompt, \"\")\n",
    "\n",
    "# the model will just keep generating, so only grab the first one\n",
    "# cleaned = cleaned.split(\"\\n\\n\")[0]\n",
    "print(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/test-fs2/project/mamba-hidden-states/mamba_custom/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from training.train_mamba_with_context import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(args):\n",
    "        \n",
    "    model = MambaLMHeadModel.from_pretrained(args.model, dtype=torch.bfloat16, device=\"cuda\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer)\n",
    "    tokenizer.eos_token = \"<|endoftext|>\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    data_module = SFTDataModule(\n",
    "        tokenizer=tokenizer,\n",
    "        data_path=args.data_path,\n",
    "    )\n",
    "\n",
    "    trainer = MambaTrainer(\n",
    "        model=model,\n",
    "        train_dataset=data_module.dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        args=TrainingArguments(\n",
    "            learning_rate=args.learning_rate,\n",
    "            num_train_epochs=args.num_epochs,\n",
    "            per_device_train_batch_size=args.batch_size,\n",
    "            gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "            optim=args.optim,\n",
    "            output_dir=args.output,\n",
    "            save_total_limit=2,\n",
    "            logging_steps=50,\n",
    "            save_steps=500,\n",
    "        ),\n",
    "        data_collator=data_module.data_collator,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(args.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--model\", type=str, default=\"state-spaces/mamba-130m\")\n",
    "parser.add_argument(\"--output\", type=str, default=\"output\")\n",
    "parser.add_argument(\"--tokenizer\", type=str, default=\"EleutherAI/gpt-neox-20b\")\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=5e-4)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=8)\n",
    "parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=1)\n",
    "parser.add_argument(\"--optim\", type=str, default=\"adamw_torch\")\n",
    "parser.add_argument(\"--data_path\", type=str, default=\"squad\")\n",
    "parser.add_argument(\"--num_epochs\", type=int, default=10)\n",
    "args = parser.parse_args('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 0 examples, preprocess...\n",
      "Tokenizing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87599/87599 [01:37<00:00, 896.27it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1671' max='219000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  1671/219000 03:17 < 7:07:57, 8.46 it/s, Epoch 0.08/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.442700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.997200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.048600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.988000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.921600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.960800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.001700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.855900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.858300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.867000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.834800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.934400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.863000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.879500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.839000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.893900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.878900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.941400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.849400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.820800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.751600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.864400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.820200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.785000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.726400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.791600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.740200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.710800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.707200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>1.847900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.786100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>1.775600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 32\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m      9\u001b[0m data_module \u001b[38;5;241m=\u001b[39m SFTDataModule(\n\u001b[1;32m     10\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     11\u001b[0m     data_path\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdata_path,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m trainer \u001b[38;5;241m=\u001b[39m MambaTrainer(\n\u001b[1;32m     15\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     16\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mdata_module\u001b[38;5;241m.\u001b[39mdataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_module\u001b[38;5;241m.\u001b[39mdata_collator,\n\u001b[1;32m     30\u001b[0m )\n\u001b[0;32m---> 32\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(args\u001b[38;5;241m.\u001b[39moutput)\n",
      "File \u001b[0;32m~/test-fs2/project/mamba-hidden-states/mamba_custom/lib/python3.10/site-packages/transformers/trainer.py:1537\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/test-fs2/project/mamba-hidden-states/mamba_custom/lib/python3.10/site-packages/transformers/trainer.py:1859\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1853\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   1854\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1857\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m-> 1859\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1860\u001b[0m ):\n\u001b[1;32m   1861\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1862\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   1863\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba_env",
   "language": "python",
   "name": "mamba_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
