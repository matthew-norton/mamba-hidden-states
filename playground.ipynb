{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "python -m venv mamba_env\n",
    "source mamba_env/bin/activate\n",
    "pip install packaging wheel torch ipykernel datasets\n",
    "pip install accelerate -U\n",
    "pip install .\n",
    "git clone https://github.com/bigcode-project/bigcode-evaluation-harness.git\n",
    "cd bigcode-evaluation-harness\n",
    "pip install -e .\n",
    "pip install flash_attn\n",
    "\n",
    "cd mamba-hidden-states\n",
    "git submodule update --init --recursive\n",
    "pip install -e 3rdparty/lm-evaluation-harness\n",
    "\n",
    "python -m ipykernel install --user --name=mamba_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, GPTNeoXForCausalLM, GPTNeoXModel\n",
    "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
    "import sys\n",
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "\n",
    "# Take in the model you want to train\n",
    "model_name = \"state-spaces/mamba-130m\"\n",
    "\n",
    "# Choose a tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "tokenizer.eos_token = \"<|endoftext|>\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenizer_pythia = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-160m-deduped\")\n",
    "tokenizer_pythia.eos_token = \"<|endoftext|>\"\n",
    "tokenizer_pythia.pad_token = tokenizer_pythia.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer, GPTNeoXModel\n",
    "\n",
    "pythia = GPTNeoXForCausalLM.from_pretrained(\n",
    "  \"EleutherAI/pythia-410m-deduped\",\n",
    "  #output_hidden_states=True,\n",
    "  #revision=\"step3000\",\n",
    "  #cache_dir=\"./pythia-70m-deduped/step3000\",\n",
    ").to(torch.device('cuda:0'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mamba = MambaLMHeadModel.from_pretrained(\n",
    "    model_name, \n",
    "    device=\"cuda\", \n",
    "    dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the user input from the command line\n",
    "user_message = \"Give me three steps to improve my diet, and include some evidence\"#input(\"\\n> \")\n",
    "\n",
    "# Create a prompt\n",
    "n_shot_prompting = [\n",
    "    {\n",
    "        \"question\": \"What is the capital of France?\",\n",
    "        \"answer\": \"Paris\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Who invented the segway?\",\n",
    "        \"answer\": \"Dean Kamen\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the fastest animal?\",\n",
    "        \"answer\": \"Cheetah\"\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt = f\"You are a Trivia QA bot.\\nAnswer the following question succinctly and accurately.\"\n",
    "prompt = f\"{prompt}\\n\\n\" + \"\\n\\n\".join([f\"Q: {p['question']}\\nA: {p['answer']}\" for p in n_shot_prompting])\n",
    "prompt = f\"{prompt}\\n\\nQ: {user_message}\\nA:\"\n",
    "\n",
    "# Debug print to make sure our prompt looks good\n",
    "print(prompt)\n",
    "\n",
    "# Encode the text to token IDs\n",
    "input_ids = torch.LongTensor([tokenizer.encode(prompt)]).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an output sequence of tokens given the input\n",
    "# \"out\" will contain the raw token ids as integers\n",
    "out = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=256,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pythia_out = pythia(\n",
    "    input_ids=input_ids,\n",
    "    output_hidden_states=True\n",
    ")\n",
    "\n",
    "mamba_out = mamba(\n",
    "    input_ids=input_ids,\n",
    "    output_hidden_states=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_loss = (pythia_out.logits.softmax(dim=2)[:,:,:50280] - mamba_out.logits.softmax(dim=2)).norm(dim=2).mean()\n",
    "teacher_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import math\n",
    "mu = 0\n",
    "std = math.sqrt(1.0/mamba_out.hidden_states[0].shape[-1])\n",
    "size = (1, pythia_out.hidden_states[0].shape[-1], mamba_out.hidden_states[0].shape[-1])\n",
    "W = torch.normal(0, std, size).to(torch.device('cuda:0'))\n",
    "\n",
    "\n",
    "F.cosine_similarity(pythia_out.hidden_states[0]@ W,  mamba_out.hidden_states[0], dim=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,X in enumerate(pythia_out.hidden_states):\n",
    "    print(k, X.shape)\n",
    "for k,X in enumerate(mamba_out.hidden_states):\n",
    "    print(k, X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you must use the tokenizer to decode them back into strings\n",
    "decoded = tokenizer.batch_decode(out)[0]\n",
    "print(\"=\"*80)\n",
    "print(decoded)\n",
    "# out returns the whole sequence plus the original\n",
    "cleaned = decoded.replace(prompt, \"\")\n",
    "\n",
    "# the model will just keep generating, so only grab the first one\n",
    "# cleaned = cleaned.split(\"\\n\\n\")[0]\n",
    "print(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/test-fs2/project/mamba-hidden-states/mambaphi/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from training.train_mamba_with_pythia import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(args):\n",
    "        \n",
    "    model = MambaLMHeadModel.from_pretrained(args.model, dtype=torch.bfloat16, device=\"cuda\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer)\n",
    "    tokenizer.eos_token = \"<|endoftext|>\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    data_module = SFTDataModule(\n",
    "        tokenizer=tokenizer,\n",
    "        data_path=args.data_path,\n",
    "    )\n",
    "\n",
    "    trainer = MambaTrainer(\n",
    "        model=model,\n",
    "        train_dataset=data_module.dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        args=TrainingArguments(\n",
    "            learning_rate=args.learning_rate,\n",
    "            num_train_epochs=args.num_epochs,\n",
    "            per_device_train_batch_size=args.batch_size,\n",
    "            gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "            optim=args.optim,\n",
    "            output_dir=args.output,\n",
    "            save_total_limit=2,\n",
    "            logging_steps=50,\n",
    "            save_steps=500,\n",
    "        ),\n",
    "        data_collator=data_module.data_collator,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(args.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--model\", type=str, default=\"state-spaces/mamba-130m\")\n",
    "parser.add_argument(\"--output\", type=str, default=\"output\")\n",
    "parser.add_argument(\"--tokenizer\", type=str, default=\"EleutherAI/gpt-neox-20b\")\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=5e-4)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=4)\n",
    "parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=1)\n",
    "parser.add_argument(\"--optim\", type=str, default=\"adamw_torch\")\n",
    "parser.add_argument(\"--data_path\", type=str, default=\"squad\")\n",
    "parser.add_argument(\"--num_epochs\", type=int, default=10)\n",
    "args = parser.parse_args('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 0 examples, preprocess...\n",
      "Tokenizing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87599/87599 [01:57<00:00, 746.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3749' max='438000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3749/438000 22:00 < 42:30:00, 2.84 it/s, Epoch 0.09/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.279300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.242800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.301800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.264300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.227300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.267300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.249800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.214700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.216000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.265800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.219800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.222200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.211600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.169500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.235200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.158800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.202700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.189000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.189700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.210300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.232300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.218900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.202700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.331400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.310400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.313400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.296900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.328900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.292700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.359700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.336400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.294500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.277700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.310000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.310800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.278900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.290400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.260200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.289600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.304800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.279800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.326600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.271000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.277600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.259100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.277300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.298700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.304300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.257400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.291700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.296800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.253600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.286400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.287000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.272900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.278300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.242000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.264500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.305200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.269400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.273300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.262300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.298300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.278200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.255000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.260800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.279800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.261500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.255100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.273500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.250900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.271300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.261200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.285400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 32\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m      9\u001b[0m data_module \u001b[38;5;241m=\u001b[39m SFTDataModule(\n\u001b[1;32m     10\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     11\u001b[0m     data_path\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdata_path,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m trainer \u001b[38;5;241m=\u001b[39m MambaTrainer(\n\u001b[1;32m     15\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     16\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mdata_module\u001b[38;5;241m.\u001b[39mdataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_module\u001b[38;5;241m.\u001b[39mdata_collator,\n\u001b[1;32m     30\u001b[0m )\n\u001b[0;32m---> 32\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(args\u001b[38;5;241m.\u001b[39moutput)\n",
      "File \u001b[0;32m~/test-fs2/project/mamba-hidden-states/mambaphi/lib/python3.10/site-packages/transformers/trainer.py:1537\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/test-fs2/project/mamba-hidden-states/mambaphi/lib/python3.10/site-packages/transformers/trainer.py:1859\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1853\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   1854\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1857\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m-> 1859\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1860\u001b[0m ):\n\u001b[1;32m   1861\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1862\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   1863\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mambaphi",
   "language": "python",
   "name": "mambaphi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
