{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to do\n",
    "check to see why we need to pass an attention mask when calling model.generate for the pythia model.\n",
    "maybe we also need to pass an attention mask to pythia (and phi) when training on it's internal representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/mambafs/mamba-hidden-states\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/bigscience-workshop/promptsource.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting packaging\n",
      "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m684.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting wheel\n",
      "  Downloading wheel-0.42.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.4/65.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torch\n",
      "  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting ipykernel\n",
      "  Downloading ipykernel-6.28.0-py3-none-any.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.1/114.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting datasets\n",
      "  Downloading datasets-2.16.1-py3-none-any.whl (507 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sympy\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting fsspec\n",
      "  Downloading fsspec-2023.12.2-py3-none-any.whl (168 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.0/169.0 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting jinja2\n",
      "  Downloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.2/133.2 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting networkx\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting typing-extensions\n",
      "  Downloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1\n",
      "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.1.0\n",
      "  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12\n",
      "  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting jupyter-core!=5.0.*,>=4.12\n",
      "  Downloading jupyter_core-5.7.1-py3-none-any.whl (28 kB)\n",
      "Collecting comm>=0.1.1\n",
      "  Downloading comm-0.2.1-py3-none-any.whl (7.2 kB)\n",
      "Collecting jupyter-client>=6.1.12\n",
      "  Downloading jupyter_client-8.6.0-py3-none-any.whl (105 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.9/105.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting matplotlib-inline>=0.1\n",
      "  Downloading matplotlib_inline-0.1.6-py3-none-any.whl (9.4 kB)\n",
      "Collecting debugpy>=1.6.5\n",
      "  Downloading debugpy-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nest-asyncio\n",
      "  Downloading nest_asyncio-1.5.8-py3-none-any.whl (5.3 kB)\n",
      "Collecting pyzmq>=24\n",
      "  Downloading pyzmq-25.1.2-cp310-cp310-manylinux_2_28_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ipython>=7.23.1\n",
      "  Downloading ipython-8.20.0-py3-none-any.whl (809 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m809.2/809.2 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting traitlets>=5.4.0\n",
      "  Downloading traitlets-5.14.1-py3-none-any.whl (85 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tornado>=6.1\n",
      "  Downloading tornado-6.4-cp38-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.4/435.4 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting psutil\n",
      "  Downloading psutil-5.9.7-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (285 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.5/285.5 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting huggingface-hub>=0.19.4\n",
      "  Downloading huggingface_hub-0.20.2-py3-none-any.whl (330 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.3/330.3 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec[http]<=2023.10.0,>=2023.1.0\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow-hotfix\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Collecting tqdm>=4.62.1\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohttp\n",
      "  Downloading aiohttp-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests>=2.19.0\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=8.0.0\n",
      "  Downloading pyarrow-14.0.2-cp310-cp310-manylinux_2_28_x86_64.whl (38.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting dill<0.3.8,>=0.3.0\n",
      "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas\n",
      "  Downloading pandas-2.1.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m705.5/705.5 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multiprocess\n",
      "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy>=1.17\n",
      "  Downloading numpy-1.26.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.5/239.5 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m301.6/301.6 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting attrs>=17.3.0\n",
      "  Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting prompt-toolkit<3.1.0,>=3.0.41\n",
      "  Downloading prompt_toolkit-3.0.43-py3-none-any.whl (386 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.1/386.1 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pygments>=2.4.0\n",
      "  Downloading pygments-2.17.2-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pexpect>4.3\n",
      "  Downloading pexpect-4.9.0-py2.py3-none-any.whl (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting stack-data\n",
      "  Downloading stack_data-0.6.3-py3-none-any.whl (24 kB)\n",
      "Collecting exceptiongroup\n",
      "  Downloading exceptiongroup-1.2.0-py3-none-any.whl (16 kB)\n",
      "Collecting jedi>=0.16\n",
      "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting decorator\n",
      "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
      "Collecting python-dateutil>=2.8.2\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.7/247.7 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting platformdirs>=2.5\n",
      "  Downloading platformdirs-4.1.0-py3-none-any.whl (17 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2023.11.17-py3-none-any.whl (162 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.5/162.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.1.0-py3-none-any.whl (104 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.6/104.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.1/142.1 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting idna<4,>=2.5\n",
      "  Downloading idna-3.6-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.5/502.5 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tzdata>=2022.1\n",
      "  Downloading tzdata-2023.4-py2.py3-none-any.whl (346 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.6/346.6 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting mpmath>=0.19\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting parso<0.9.0,>=0.8.3\n",
      "  Downloading parso-0.8.3-py2.py3-none-any.whl (100 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.8/100.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ptyprocess>=0.5\n",
      "  Downloading ptyprocess-0.7.0-py2.py3-none-any.whl (13 kB)\n",
      "Collecting wcwidth\n",
      "  Downloading wcwidth-0.2.13-py2.py3-none-any.whl (34 kB)\n",
      "Collecting six>=1.5\n",
      "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting asttokens>=2.1.0\n",
      "  Downloading asttokens-2.4.1-py2.py3-none-any.whl (27 kB)\n",
      "Collecting pure-eval\n",
      "  Downloading pure_eval-0.2.2-py3-none-any.whl (11 kB)\n",
      "Collecting executing>=1.2.0\n",
      "  Downloading executing-2.0.1-py2.py3-none-any.whl (24 kB)\n",
      "Installing collected packages: wcwidth, pytz, pure-eval, ptyprocess, mpmath, xxhash, wheel, urllib3, tzdata, typing-extensions, traitlets, tqdm, tornado, sympy, six, pyzmq, pyyaml, pygments, pyarrow-hotfix, psutil, prompt-toolkit, platformdirs, pexpect, parso, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, nest-asyncio, multidict, MarkupSafe, idna, fsspec, frozenlist, filelock, executing, exceptiongroup, dill, decorator, debugpy, charset-normalizer, certifi, attrs, async-timeout, yarl, triton, requests, python-dateutil, pyarrow, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, matplotlib-inline, jupyter-core, jinja2, jedi, comm, asttokens, aiosignal, stack-data, pandas, nvidia-cusolver-cu12, jupyter-client, huggingface-hub, aiohttp, torch, ipython, ipykernel, datasets\n",
      "Successfully installed MarkupSafe-2.1.3 aiohttp-3.9.1 aiosignal-1.3.1 asttokens-2.4.1 async-timeout-4.0.3 attrs-23.2.0 certifi-2023.11.17 charset-normalizer-3.3.2 comm-0.2.1 datasets-2.16.1 debugpy-1.8.0 decorator-5.1.1 dill-0.3.7 exceptiongroup-1.2.0 executing-2.0.1 filelock-3.13.1 frozenlist-1.4.1 fsspec-2023.10.0 huggingface-hub-0.20.2 idna-3.6 ipykernel-6.28.0 ipython-8.20.0 jedi-0.19.1 jinja2-3.1.3 jupyter-client-8.6.0 jupyter-core-5.7.1 matplotlib-inline-0.1.6 mpmath-1.3.0 multidict-6.0.4 multiprocess-0.70.15 nest-asyncio-1.5.8 networkx-3.2.1 numpy-1.26.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 packaging-23.2 pandas-2.1.4 parso-0.8.3 pexpect-4.9.0 platformdirs-4.1.0 prompt-toolkit-3.0.43 psutil-5.9.7 ptyprocess-0.7.0 pure-eval-0.2.2 pyarrow-14.0.2 pyarrow-hotfix-0.6 pygments-2.17.2 python-dateutil-2.8.2 pytz-2023.3.post1 pyyaml-6.0.1 pyzmq-25.1.2 requests-2.31.0 six-1.16.0 stack-data-0.6.3 sympy-1.12 torch-2.1.2 tornado-6.4 tqdm-4.66.1 traitlets-5.14.1 triton-2.1.0 typing-extensions-4.9.0 tzdata-2023.4 urllib3-2.1.0 wcwidth-0.2.13 wheel-0.42.0 xxhash-3.4.1 yarl-1.9.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-0.26.1-py3-none-any.whl (270 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.10.0 in ./mamba_env/lib/python3.10/site-packages (from accelerate) (2.1.2)\n",
      "Requirement already satisfied: huggingface-hub in ./mamba_env/lib/python3.10/site-packages (from accelerate) (0.20.2)\n",
      "Requirement already satisfied: pyyaml in ./mamba_env/lib/python3.10/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./mamba_env/lib/python3.10/site-packages (from accelerate) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in ./mamba_env/lib/python3.10/site-packages (from accelerate) (23.2)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil in ./mamba_env/lib/python3.10/site-packages (from accelerate) (5.9.7)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in ./mamba_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.18.1)\n",
      "Requirement already satisfied: jinja2 in ./mamba_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./mamba_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: filelock in ./mamba_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./mamba_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in ./mamba_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./mamba_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./mamba_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./mamba_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: fsspec in ./mamba_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2023.10.0)\n",
      "Requirement already satisfied: sympy in ./mamba_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./mamba_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./mamba_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./mamba_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./mamba_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./mamba_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: networkx in ./mamba_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: typing-extensions in ./mamba_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./mamba_env/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.3.101)\n",
      "Requirement already satisfied: requests in ./mamba_env/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./mamba_env/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./mamba_env/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./mamba_env/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./mamba_env/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./mamba_env/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./mamba_env/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./mamba_env/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Installing collected packages: safetensors, accelerate\n",
      "Successfully installed accelerate-0.26.1 safetensors-0.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /home/mamba-hidden-states\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: torch in ./mamba_env/lib/python3.10/site-packages (from mamba-ssm==1.1.1) (2.1.2)\n",
      "Requirement already satisfied: packaging in ./mamba_env/lib/python3.10/site-packages (from mamba-ssm==1.1.1) (23.2)\n",
      "Collecting ninja\n",
      "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting einops\n",
      "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: triton in ./mamba_env/lib/python3.10/site-packages (from mamba-ssm==1.1.1) (2.1.0)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.36.2-py3-none-any.whl (8.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting causal_conv1d>=1.1.0\n",
      "  Downloading causal_conv1d-1.1.1.tar.gz (6.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting buildtools\n",
      "  Downloading buildtools-1.0.6.tar.gz (446 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m446.5/446.5 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./mamba_env/lib/python3.10/site-packages (from torch->mamba-ssm==1.1.1) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./mamba_env/lib/python3.10/site-packages (from torch->mamba-ssm==1.1.1) (11.0.2.54)\n",
      "Requirement already satisfied: typing-extensions in ./mamba_env/lib/python3.10/site-packages (from torch->mamba-ssm==1.1.1) (4.9.0)\n",
      "Requirement already satisfied: filelock in ./mamba_env/lib/python3.10/site-packages (from torch->mamba-ssm==1.1.1) (3.13.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./mamba_env/lib/python3.10/site-packages (from torch->mamba-ssm==1.1.1) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in ./mamba_env/lib/python3.10/site-packages (from torch->mamba-ssm==1.1.1) (2.18.1)\n",
      "Requirement already satisfied: fsspec in ./mamba_env/lib/python3.10/site-packages (from torch->mamba-ssm==1.1.1) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./mamba_env/lib/python3.10/site-packages (from torch->mamba-ssm==1.1.1) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./mamba_env/lib/python3.10/site-packages (from torch->mamba-ssm==1.1.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./mamba_env/lib/python3.10/site-packages (from torch->mamba-ssm==1.1.1) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./mamba_env/lib/python3.10/site-packages (from torch->mamba-ssm==1.1.1) (12.1.105)\n",
      "Requirement already satisfied: jinja2 in ./mamba_env/lib/python3.10/site-packages (from torch->mamba-ssm==1.1.1) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./mamba_env/lib/python3.10/site-packages (from torch->mamba-ssm==1.1.1) (12.1.3.1)\n",
      "Requirement already satisfied: sympy in ./mamba_env/lib/python3.10/site-packages (from torch->mamba-ssm==1.1.1) (1.12)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./mamba_env/lib/python3.10/site-packages (from torch->mamba-ssm==1.1.1) (12.1.105)\n",
      "Requirement already satisfied: networkx in ./mamba_env/lib/python3.10/site-packages (from torch->mamba-ssm==1.1.1) (3.2.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./mamba_env/lib/python3.10/site-packages (from torch->mamba-ssm==1.1.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./mamba_env/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->mamba-ssm==1.1.1) (12.3.101)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in ./mamba_env/lib/python3.10/site-packages (from transformers->mamba-ssm==1.1.1) (0.20.2)\n",
      "Collecting tokenizers<0.19,>=0.14\n",
      "  Downloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in ./mamba_env/lib/python3.10/site-packages (from transformers->mamba-ssm==1.1.1) (4.66.1)\n",
      "Requirement already satisfied: requests in ./mamba_env/lib/python3.10/site-packages (from transformers->mamba-ssm==1.1.1) (2.31.0)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.0/774.0 kB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in ./mamba_env/lib/python3.10/site-packages (from transformers->mamba-ssm==1.1.1) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./mamba_env/lib/python3.10/site-packages (from transformers->mamba-ssm==1.1.1) (1.26.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./mamba_env/lib/python3.10/site-packages (from transformers->mamba-ssm==1.1.1) (6.0.1)\n",
      "Collecting sqlalchemy\n",
      "  Downloading SQLAlchemy-2.0.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting argparse\n",
      "  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting twisted\n",
      "  Downloading twisted-23.10.0-py3-none-any.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting simplejson\n",
      "  Downloading simplejson-3.19.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting furl\n",
      "  Downloading furl-2.1.3-py2.py3-none-any.whl (20 kB)\n",
      "Collecting docopt\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: python-dateutil in ./mamba_env/lib/python3.10/site-packages (from buildtools->causal_conv1d>=1.1.0->mamba-ssm==1.1.1) (2.8.2)\n",
      "Collecting redo\n",
      "  Downloading redo-2.0.4-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./mamba_env/lib/python3.10/site-packages (from jinja2->torch->mamba-ssm==1.1.1) (2.1.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./mamba_env/lib/python3.10/site-packages (from requests->transformers->mamba-ssm==1.1.1) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./mamba_env/lib/python3.10/site-packages (from requests->transformers->mamba-ssm==1.1.1) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./mamba_env/lib/python3.10/site-packages (from requests->transformers->mamba-ssm==1.1.1) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./mamba_env/lib/python3.10/site-packages (from requests->transformers->mamba-ssm==1.1.1) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./mamba_env/lib/python3.10/site-packages (from sympy->torch->mamba-ssm==1.1.1) (1.3.0)\n",
      "Requirement already satisfied: six>=1.8.0 in ./mamba_env/lib/python3.10/site-packages (from furl->buildtools->causal_conv1d>=1.1.0->mamba-ssm==1.1.1) (1.16.0)\n",
      "Collecting orderedmultidict>=1.0.1\n",
      "  Downloading orderedmultidict-1.0.1-py2.py3-none-any.whl (11 kB)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Downloading greenlet-3.0.3-cp310-cp310-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (616 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m616.0/616.0 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting automat>=0.8.0\n",
      "  Downloading Automat-22.10.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting constantly>=15.1\n",
      "  Downloading constantly-23.10.4-py3-none-any.whl (13 kB)\n",
      "Collecting hyperlink>=17.1.1\n",
      "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting incremental>=22.10.0\n",
      "  Downloading incremental-22.10.0-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: attrs>=21.3.0 in ./mamba_env/lib/python3.10/site-packages (from twisted->buildtools->causal_conv1d>=1.1.0->mamba-ssm==1.1.1) (23.2.0)\n",
      "Collecting zope-interface>=5\n",
      "  Downloading zope.interface-6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (247 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.1/247.1 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in ./mamba_env/lib/python3.10/site-packages (from zope-interface>=5->twisted->buildtools->causal_conv1d>=1.1.0->mamba-ssm==1.1.1) (65.5.0)\n",
      "Building wheels for collected packages: mamba-ssm, causal_conv1d, buildtools, docopt\n",
      "  Building wheel for mamba-ssm (setup.py): started\n",
      "  Building wheel for mamba-ssm (setup.py): finished with status 'done'\n",
      "  Created wheel for mamba-ssm: filename=mamba_ssm-1.1.1-cp310-cp310-linux_x86_64.whl size=137574741 sha256=998cf941c62596ab13766c8144a1a1b269915f501d95e600b28b85627fdf6dee\n",
      "  Stored in directory: /home/.cache/pip/wheels/58/f1/15/814fd85fd65f4bb2f2f4ec0b491b9717221cb09400da2fe7f8\n",
      "  Building wheel for causal_conv1d (setup.py): started\n",
      "  Building wheel for causal_conv1d (setup.py): finished with status 'done'\n",
      "  Created wheel for causal_conv1d: filename=causal_conv1d-1.1.1-cp310-cp310-linux_x86_64.whl size=13747478 sha256=21624d55bd83e013a1df9ef514b6690aabbce45c0dd1d1b978a772edbd6c1c85\n",
      "  Stored in directory: /home/.cache/pip/wheels/4f/9e/69/9060a1871f461dfca87b667350f5728d44c555f98dacaf04ff\n",
      "  Building wheel for buildtools (setup.py): started\n",
      "  Building wheel for buildtools (setup.py): finished with status 'done'\n",
      "  Created wheel for buildtools: filename=buildtools-1.0.6-py3-none-any.whl size=512343 sha256=3b9d4b57bde1a873b3d7f7b45c5ef94f5f3b1a6c39c233d15d885ca0669b37a3\n",
      "  Stored in directory: /home/.cache/pip/wheels/90/e9/2a/625d99dffa430d0b4293d3d386f63e0eb8edeeb54f3f29d208\n",
      "  Building wheel for docopt (setup.py): started\n",
      "  Building wheel for docopt (setup.py): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=6ea432969afe1cb94761f9783904398bfe7139695956facb3c1063374290ab9d\n",
      "  Stored in directory: /home/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
      "Successfully built mamba-ssm causal_conv1d buildtools docopt\n",
      "Installing collected packages: redo, ninja, incremental, docopt, argparse, zope-interface, simplejson, regex, orderedmultidict, hyperlink, greenlet, einops, constantly, automat, twisted, sqlalchemy, furl, tokenizers, buildtools, transformers, causal_conv1d, mamba-ssm\n",
      "Successfully installed argparse-1.4.0 automat-22.10.0 buildtools-1.0.6 causal_conv1d-1.1.1 constantly-23.10.4 docopt-0.6.2 einops-0.7.0 furl-2.1.3 greenlet-3.0.3 hyperlink-21.0.0 incremental-22.10.0 mamba-ssm-1.1.1 ninja-1.11.1.1 orderedmultidict-1.0.1 redo-2.0.4 regex-2023.12.25 simplejson-3.19.2 sqlalchemy-2.0.25 tokenizers-0.15.0 transformers-4.36.2 twisted-23.10.0 zope-interface-6.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Cloning into 'bigcode-evaluation-harness'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/mamba-hidden-states/3rdparty/bigcode-evaluation-harness\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: transformers>=4.25.1 in ./mamba_env/lib/python3.10/site-packages (from bigcode-eval==0.0.0) (4.36.2)\n",
      "Requirement already satisfied: accelerate>=0.13.2 in ./mamba_env/lib/python3.10/site-packages (from bigcode-eval==0.0.0) (0.26.1)\n",
      "Requirement already satisfied: datasets>=2.6.1 in ./mamba_env/lib/python3.10/site-packages (from bigcode-eval==0.0.0) (2.16.1)\n",
      "Collecting evaluate>=0.3.0\n",
      "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m944.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyext==0.7\n",
      "  Downloading pyext-0.7.tar.gz (7.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting mosestokenizer==1.0.0\n",
      "  Downloading mosestokenizer-1.0.0-py3-none-any.whl (51 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.2/51.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface_hub>=0.11.1 in ./mamba_env/lib/python3.10/site-packages (from bigcode-eval==0.0.0) (0.20.2)\n",
      "Collecting fsspec<2023.10.0\n",
      "  Downloading fsspec-2023.9.2-py3-none-any.whl (173 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.4/173.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting openfile\n",
      "  Downloading openfile-0.0.7-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: docopt in ./mamba_env/lib/python3.10/site-packages (from mosestokenizer==1.0.0->bigcode-eval==0.0.0) (0.6.2)\n",
      "Collecting toolwrapper\n",
      "  Downloading toolwrapper-2.1.0.tar.gz (3.2 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: psutil in ./mamba_env/lib/python3.10/site-packages (from accelerate>=0.13.2->bigcode-eval==0.0.0) (5.9.7)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./mamba_env/lib/python3.10/site-packages (from accelerate>=0.13.2->bigcode-eval==0.0.0) (0.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./mamba_env/lib/python3.10/site-packages (from accelerate>=0.13.2->bigcode-eval==0.0.0) (23.2)\n",
      "Requirement already satisfied: pyyaml in ./mamba_env/lib/python3.10/site-packages (from accelerate>=0.13.2->bigcode-eval==0.0.0) (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./mamba_env/lib/python3.10/site-packages (from accelerate>=0.13.2->bigcode-eval==0.0.0) (1.26.3)\n",
      "Requirement already satisfied: torch>=1.10.0 in ./mamba_env/lib/python3.10/site-packages (from accelerate>=0.13.2->bigcode-eval==0.0.0) (2.1.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in ./mamba_env/lib/python3.10/site-packages (from datasets>=2.6.1->bigcode-eval==0.0.0) (14.0.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in ./mamba_env/lib/python3.10/site-packages (from datasets>=2.6.1->bigcode-eval==0.0.0) (4.66.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./mamba_env/lib/python3.10/site-packages (from datasets>=2.6.1->bigcode-eval==0.0.0) (2.31.0)\n",
      "Requirement already satisfied: multiprocess in ./mamba_env/lib/python3.10/site-packages (from datasets>=2.6.1->bigcode-eval==0.0.0) (0.70.15)\n",
      "Requirement already satisfied: filelock in ./mamba_env/lib/python3.10/site-packages (from datasets>=2.6.1->bigcode-eval==0.0.0) (3.13.1)\n",
      "Requirement already satisfied: aiohttp in ./mamba_env/lib/python3.10/site-packages (from datasets>=2.6.1->bigcode-eval==0.0.0) (3.9.1)\n",
      "Requirement already satisfied: xxhash in ./mamba_env/lib/python3.10/site-packages (from datasets>=2.6.1->bigcode-eval==0.0.0) (3.4.1)\n",
      "Requirement already satisfied: pandas in ./mamba_env/lib/python3.10/site-packages (from datasets>=2.6.1->bigcode-eval==0.0.0) (2.1.4)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in ./mamba_env/lib/python3.10/site-packages (from datasets>=2.6.1->bigcode-eval==0.0.0) (2023.10.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in ./mamba_env/lib/python3.10/site-packages (from datasets>=2.6.1->bigcode-eval==0.0.0) (0.3.7)\n",
      "Requirement already satisfied: pyarrow-hotfix in ./mamba_env/lib/python3.10/site-packages (from datasets>=2.6.1->bigcode-eval==0.0.0) (0.6)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./mamba_env/lib/python3.10/site-packages (from huggingface_hub>=0.11.1->bigcode-eval==0.0.0) (4.9.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./mamba_env/lib/python3.10/site-packages (from transformers>=4.25.1->bigcode-eval==0.0.0) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in ./mamba_env/lib/python3.10/site-packages (from transformers>=4.25.1->bigcode-eval==0.0.0) (0.15.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./mamba_env/lib/python3.10/site-packages (from aiohttp->datasets>=2.6.1->bigcode-eval==0.0.0) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./mamba_env/lib/python3.10/site-packages (from aiohttp->datasets>=2.6.1->bigcode-eval==0.0.0) (1.9.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./mamba_env/lib/python3.10/site-packages (from aiohttp->datasets>=2.6.1->bigcode-eval==0.0.0) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./mamba_env/lib/python3.10/site-packages (from aiohttp->datasets>=2.6.1->bigcode-eval==0.0.0) (23.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./mamba_env/lib/python3.10/site-packages (from aiohttp->datasets>=2.6.1->bigcode-eval==0.0.0) (4.0.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./mamba_env/lib/python3.10/site-packages (from aiohttp->datasets>=2.6.1->bigcode-eval==0.0.0) (1.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./mamba_env/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.6.1->bigcode-eval==0.0.0) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./mamba_env/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.6.1->bigcode-eval==0.0.0) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./mamba_env/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.6.1->bigcode-eval==0.0.0) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./mamba_env/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.6.1->bigcode-eval==0.0.0) (3.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./mamba_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.13.2->bigcode-eval==0.0.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./mamba_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.13.2->bigcode-eval==0.0.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./mamba_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.13.2->bigcode-eval==0.0.0) (11.0.2.54)\n",
      "Requirement already satisfied: networkx in ./mamba_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.13.2->bigcode-eval==0.0.0) (3.2.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./mamba_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.13.2->bigcode-eval==0.0.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./mamba_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.13.2->bigcode-eval==0.0.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./mamba_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.13.2->bigcode-eval==0.0.0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./mamba_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.13.2->bigcode-eval==0.0.0) (12.1.0.106)\n",
      "Requirement already satisfied: triton==2.1.0 in ./mamba_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.13.2->bigcode-eval==0.0.0) (2.1.0)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./mamba_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.13.2->bigcode-eval==0.0.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./mamba_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.13.2->bigcode-eval==0.0.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in ./mamba_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.13.2->bigcode-eval==0.0.0) (2.18.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./mamba_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.13.2->bigcode-eval==0.0.0) (12.1.105)\n",
      "Requirement already satisfied: sympy in ./mamba_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.13.2->bigcode-eval==0.0.0) (1.12)\n",
      "Requirement already satisfied: jinja2 in ./mamba_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate>=0.13.2->bigcode-eval==0.0.0) (3.1.3)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./mamba_env/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate>=0.13.2->bigcode-eval==0.0.0) (12.3.101)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./mamba_env/lib/python3.10/site-packages (from pandas->datasets>=2.6.1->bigcode-eval==0.0.0) (2023.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./mamba_env/lib/python3.10/site-packages (from pandas->datasets>=2.6.1->bigcode-eval==0.0.0) (2023.3.post1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./mamba_env/lib/python3.10/site-packages (from pandas->datasets>=2.6.1->bigcode-eval==0.0.0) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in ./mamba_env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.6.1->bigcode-eval==0.0.0) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./mamba_env/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate>=0.13.2->bigcode-eval==0.0.0) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./mamba_env/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate>=0.13.2->bigcode-eval==0.0.0) (1.3.0)\n",
      "Building wheels for collected packages: pyext, toolwrapper\n",
      "  Building wheel for pyext (setup.py): started\n",
      "  Building wheel for pyext (setup.py): finished with status 'done'\n",
      "  Created wheel for pyext: filename=pyext-0.7-py3-none-any.whl size=7222 sha256=08631eb12219a113c01de64b4db25d71e5c4a17b00c65a4dcff768bcb79007a5\n",
      "  Stored in directory: /home/.cache/pip/wheels/09/95/a9/f3f15c5e52dec7912c332ae503e82fd680e576bf336437f002\n",
      "  Building wheel for toolwrapper (setup.py): started\n",
      "  Building wheel for toolwrapper (setup.py): finished with status 'done'\n",
      "  Created wheel for toolwrapper: filename=toolwrapper-2.1.0-py3-none-any.whl size=3338 sha256=e71833ba23626e8c029ba0756ffef9d5c743349bb135796c925b7bc5034bbfcc\n",
      "  Stored in directory: /home/.cache/pip/wheels/e1/af/b1/99b57a06dda78fdcee86d2e22c64743f3b8df8f31cfc04baf7\n",
      "Successfully built pyext toolwrapper\n",
      "Installing collected packages: toolwrapper, pyext, openfile, mosestokenizer, fsspec, responses, evaluate, bigcode-eval\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.10.0\n",
      "    Uninstalling fsspec-2023.10.0:\n",
      "      Successfully uninstalled fsspec-2023.10.0\n",
      "  Running setup.py develop for bigcode-eval\n",
      "Successfully installed bigcode-eval-0.0.0 evaluate-0.4.1 fsspec-2023.9.2 mosestokenizer-1.0.0 openfile-0.0.7 pyext-0.7 responses-0.18.0 toolwrapper-2.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Submodule '3rdparty/lm-evaluation-harness' (https://github.com/EleutherAI/lm-evaluation-harness/) registered for path '3rdparty/lm-evaluation-harness'\n",
      "Cloning into '/home/mamba-hidden-states/3rdparty/lm-evaluation-harness'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submodule path '3rdparty/lm-evaluation-harness': checked out 'a35206191acac1776761e737b66e0d04975d21b9'\n",
      "Obtaining file:///home/mamba-hidden-states/3rdparty/lm-evaluation-harness\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Checking if build backend supports build_editable: started\n",
      "  Checking if build backend supports build_editable: finished with status 'done'\n",
      "  Getting requirements to build editable: started\n",
      "  Getting requirements to build editable: finished with status 'done'\n",
      "  Preparing editable metadata (pyproject.toml): started\n",
      "  Preparing editable metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: datasets>=2.0.0 in ./mamba_env/lib/python3.10/site-packages (from lm_eval==1.0.0) (2.16.1)\n",
      "Collecting sacrebleu>=1.5.0\n",
      "  Downloading sacrebleu-2.4.0-py3-none-any.whl (106 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting sqlitedict\n",
      "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: accelerate>=0.21.0 in ./mamba_env/lib/python3.10/site-packages (from lm_eval==1.0.0) (0.26.1)\n",
      "Collecting peft>=0.2.0\n",
      "  Downloading peft-0.7.1-py3-none-any.whl (168 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting scikit-learn>=0.24.1\n",
      "  Downloading scikit_learn-1.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting numexpr\n",
      "  Downloading numexpr-2.8.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (374 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.8/374.8 kB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.8 in ./mamba_env/lib/python3.10/site-packages (from lm_eval==1.0.0) (2.1.2)\n",
      "Collecting zstandard\n",
      "  Downloading zstandard-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: transformers>=4.1 in ./mamba_env/lib/python3.10/site-packages (from lm_eval==1.0.0) (4.36.2)\n",
      "Collecting pybind11>=2.6.2\n",
      "  Downloading pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.7/227.7 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pytablewriter\n",
      "  Downloading pytablewriter-1.2.0-py3-none-any.whl (111 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.1/111.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting jsonlines\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Collecting rouge-score>=0.0.4\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting tqdm-multiprocess\n",
      "  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: evaluate in ./mamba_env/lib/python3.10/site-packages (from lm_eval==1.0.0) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./mamba_env/lib/python3.10/site-packages (from accelerate>=0.21.0->lm_eval==1.0.0) (1.26.3)\n",
      "Requirement already satisfied: huggingface-hub in ./mamba_env/lib/python3.10/site-packages (from accelerate>=0.21.0->lm_eval==1.0.0) (0.20.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./mamba_env/lib/python3.10/site-packages (from accelerate>=0.21.0->lm_eval==1.0.0) (23.2)\n",
      "Requirement already satisfied: pyyaml in ./mamba_env/lib/python3.10/site-packages (from accelerate>=0.21.0->lm_eval==1.0.0) (6.0.1)\n",
      "Requirement already satisfied: psutil in ./mamba_env/lib/python3.10/site-packages (from accelerate>=0.21.0->lm_eval==1.0.0) (5.9.7)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./mamba_env/lib/python3.10/site-packages (from accelerate>=0.21.0->lm_eval==1.0.0) (0.4.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in ./mamba_env/lib/python3.10/site-packages (from datasets>=2.0.0->lm_eval==1.0.0) (0.6)\n",
      "Requirement already satisfied: filelock in ./mamba_env/lib/python3.10/site-packages (from datasets>=2.0.0->lm_eval==1.0.0) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in ./mamba_env/lib/python3.10/site-packages (from datasets>=2.0.0->lm_eval==1.0.0) (14.0.2)\n",
      "Requirement already satisfied: pandas in ./mamba_env/lib/python3.10/site-packages (from datasets>=2.0.0->lm_eval==1.0.0) (2.1.4)\n",
      "Requirement already satisfied: xxhash in ./mamba_env/lib/python3.10/site-packages (from datasets>=2.0.0->lm_eval==1.0.0) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in ./mamba_env/lib/python3.10/site-packages (from datasets>=2.0.0->lm_eval==1.0.0) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in ./mamba_env/lib/python3.10/site-packages (from datasets>=2.0.0->lm_eval==1.0.0) (2023.9.2)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in ./mamba_env/lib/python3.10/site-packages (from datasets>=2.0.0->lm_eval==1.0.0) (0.3.7)\n",
      "Requirement already satisfied: aiohttp in ./mamba_env/lib/python3.10/site-packages (from datasets>=2.0.0->lm_eval==1.0.0) (3.9.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./mamba_env/lib/python3.10/site-packages (from datasets>=2.0.0->lm_eval==1.0.0) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in ./mamba_env/lib/python3.10/site-packages (from datasets>=2.0.0->lm_eval==1.0.0) (4.66.1)\n",
      "Requirement already satisfied: responses<0.19 in ./mamba_env/lib/python3.10/site-packages (from evaluate->lm_eval==1.0.0) (0.18.0)\n",
      "Collecting absl-py\n",
      "  Downloading absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.14.0 in ./mamba_env/lib/python3.10/site-packages (from rouge-score>=0.0.4->lm_eval==1.0.0) (1.16.0)\n",
      "Collecting tabulate>=0.8.9\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Collecting colorama\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Collecting portalocker\n",
      "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: regex in ./mamba_env/lib/python3.10/site-packages (from sacrebleu>=1.5.0->lm_eval==1.0.0) (2023.12.25)\n",
      "Collecting lxml\n",
      "  Downloading lxml-5.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m45.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hCollecting joblib>=1.1.1\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting scipy>=1.5.0\n",
      "  Downloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./mamba_env/lib/python3.10/site-packages (from torch>=1.8->lm_eval==1.0.0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./mamba_env/lib/python3.10/site-packages (from torch>=1.8->lm_eval==1.0.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./mamba_env/lib/python3.10/site-packages (from torch>=1.8->lm_eval==1.0.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./mamba_env/lib/python3.10/site-packages (from torch>=1.8->lm_eval==1.0.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./mamba_env/lib/python3.10/site-packages (from torch>=1.8->lm_eval==1.0.0) (12.1.3.1)\n",
      "Requirement already satisfied: triton==2.1.0 in ./mamba_env/lib/python3.10/site-packages (from torch>=1.8->lm_eval==1.0.0) (2.1.0)\n",
      "Requirement already satisfied: networkx in ./mamba_env/lib/python3.10/site-packages (from torch>=1.8->lm_eval==1.0.0) (3.2.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./mamba_env/lib/python3.10/site-packages (from torch>=1.8->lm_eval==1.0.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./mamba_env/lib/python3.10/site-packages (from torch>=1.8->lm_eval==1.0.0) (12.1.105)\n",
      "Requirement already satisfied: jinja2 in ./mamba_env/lib/python3.10/site-packages (from torch>=1.8->lm_eval==1.0.0) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./mamba_env/lib/python3.10/site-packages (from torch>=1.8->lm_eval==1.0.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./mamba_env/lib/python3.10/site-packages (from torch>=1.8->lm_eval==1.0.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in ./mamba_env/lib/python3.10/site-packages (from torch>=1.8->lm_eval==1.0.0) (2.18.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./mamba_env/lib/python3.10/site-packages (from torch>=1.8->lm_eval==1.0.0) (12.1.0.106)\n",
      "Requirement already satisfied: sympy in ./mamba_env/lib/python3.10/site-packages (from torch>=1.8->lm_eval==1.0.0) (1.12)\n",
      "Requirement already satisfied: typing-extensions in ./mamba_env/lib/python3.10/site-packages (from torch>=1.8->lm_eval==1.0.0) (4.9.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./mamba_env/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8->lm_eval==1.0.0) (12.3.101)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in ./mamba_env/lib/python3.10/site-packages (from transformers>=4.1->lm_eval==1.0.0) (0.15.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in ./mamba_env/lib/python3.10/site-packages (from jsonlines->lm_eval==1.0.0) (23.2.0)\n",
      "Collecting mbstrdecoder<2,>=1.0.0\n",
      "  Downloading mbstrdecoder-1.1.3-py3-none-any.whl (7.8 kB)\n",
      "Requirement already satisfied: setuptools>=38.3.0 in ./mamba_env/lib/python3.10/site-packages (from pytablewriter->lm_eval==1.0.0) (65.5.0)\n",
      "Collecting tabledata<2,>=1.3.1\n",
      "  Downloading tabledata-1.3.3-py3-none-any.whl (11 kB)\n",
      "Collecting pathvalidate<4,>=2.3.0\n",
      "  Downloading pathvalidate-3.2.0-py3-none-any.whl (23 kB)\n",
      "Collecting tcolorpy<1,>=0.0.5\n",
      "  Downloading tcolorpy-0.1.4-py3-none-any.whl (7.9 kB)\n",
      "Collecting typepy[datetime]<2,>=1.3.2\n",
      "  Downloading typepy-1.3.2-py3-none-any.whl (31 kB)\n",
      "Collecting DataProperty<2,>=1.0.1\n",
      "  Downloading DataProperty-1.0.1-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./mamba_env/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->lm_eval==1.0.0) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./mamba_env/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->lm_eval==1.0.0) (1.4.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./mamba_env/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->lm_eval==1.0.0) (4.0.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./mamba_env/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->lm_eval==1.0.0) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./mamba_env/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->lm_eval==1.0.0) (1.9.4)\n",
      "Collecting chardet<6,>=3.0.4\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3<3,>=1.21.1 in ./mamba_env/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.0.0->lm_eval==1.0.0) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./mamba_env/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.0.0->lm_eval==1.0.0) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./mamba_env/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.0.0->lm_eval==1.0.0) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./mamba_env/lib/python3.10/site-packages (from requests>=2.19.0->datasets>=2.0.0->lm_eval==1.0.0) (2023.11.17)\n",
      "Requirement already satisfied: pytz>=2018.9 in ./mamba_env/lib/python3.10/site-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==1.0.0) (2023.3.post1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.0 in ./mamba_env/lib/python3.10/site-packages (from typepy[datetime]<2,>=1.3.2->pytablewriter->lm_eval==1.0.0) (2.8.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./mamba_env/lib/python3.10/site-packages (from jinja2->torch>=1.8->lm_eval==1.0.0) (2.1.3)\n",
      "Collecting click\n",
      "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tzdata>=2022.1 in ./mamba_env/lib/python3.10/site-packages (from pandas->datasets>=2.0.0->lm_eval==1.0.0) (2023.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./mamba_env/lib/python3.10/site-packages (from sympy->torch>=1.8->lm_eval==1.0.0) (1.3.0)\n",
      "Building wheels for collected packages: lm_eval, rouge-score, sqlitedict\n",
      "  Building editable for lm_eval (pyproject.toml): started\n",
      "  Building editable for lm_eval (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for lm_eval: filename=lm_eval-1.0.0-0.editable-py3-none-any.whl size=11039 sha256=d994c86a31edf0578160f6fcd2b01c9a13ff5ac690f245dd6c4b6a63895f3976\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-do4nvuq5/wheels/86/b5/52/40bb9f43f3a51eb73d505b40f95a1a5ce9de6fe79eb2c436ab\n",
      "  Building wheel for rouge-score (setup.py): started\n",
      "  Building wheel for rouge-score (setup.py): finished with status 'done'\n",
      "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=460166739886a958faa4513df9f9de9552207e9694f1dc0bd99000b00b83d970\n",
      "  Stored in directory: /home/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "  Building wheel for sqlitedict (setup.py): started\n",
      "  Building wheel for sqlitedict (setup.py): finished with status 'done'\n",
      "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16863 sha256=dceab19ece98827c0f7bf07c5cf7fdef81066ae7270f207bc59be15e9e4eea51\n",
      "  Stored in directory: /home/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n",
      "Successfully built lm_eval rouge-score sqlitedict\n",
      "Installing collected packages: sqlitedict, zstandard, threadpoolctl, tcolorpy, tabulate, scipy, pybind11, portalocker, pathvalidate, numexpr, lxml, jsonlines, joblib, colorama, click, chardet, absl-py, tqdm-multiprocess, scikit-learn, sacrebleu, nltk, mbstrdecoder, typepy, rouge-score, peft, DataProperty, tabledata, pytablewriter, lm_eval\n",
      "Successfully installed DataProperty-1.0.1 absl-py-2.0.0 chardet-5.2.0 click-8.1.7 colorama-0.4.6 joblib-1.3.2 jsonlines-4.0.0 lm_eval-1.0.0 lxml-5.1.0 mbstrdecoder-1.1.3 nltk-3.8.1 numexpr-2.8.8 pathvalidate-3.2.0 peft-0.7.1 portalocker-2.8.2 pybind11-2.11.1 pytablewriter-1.2.0 rouge-score-0.1.2 sacrebleu-2.4.0 scikit-learn-1.3.2 scipy-1.11.4 sqlitedict-2.1.0 tabledata-1.3.3 tabulate-0.9.0 tcolorpy-0.1.4 threadpoolctl-3.2.0 tqdm-multiprocess-0.0.11 typepy-1.3.2 zstandard-0.22.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/bigscience-workshop/promptsource.git\n",
      "  Cloning https://github.com/bigscience-workshop/promptsource.git to /tmp/pip-req-build-wdhkxe02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/bigscience-workshop/promptsource.git /tmp/pip-req-build-wdhkxe02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Resolved https://github.com/bigscience-workshop/promptsource.git to commit 7dab96a3eeb3717cea633705135ebc488885d709\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting black<=21.12b0\n",
      "  Downloading black-21.12b0-py3-none-any.whl (156 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.7/156.7 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: datasets>=1.7.0 in ./mamba_env/lib/python3.10/site-packages (from promptsource==0.2.3) (2.16.1)\n",
      "Collecting flake8\n",
      "  Downloading flake8-7.0.0-py2.py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting isort==5.8.0\n",
      "  Downloading isort-5.8.0-py3-none-any.whl (103 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pytest\n",
      "  Downloading pytest-7.4.4-py3-none-any.whl (325 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.3/325.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5 in ./mamba_env/lib/python3.10/site-packages (from promptsource==0.2.3) (6.0.1)\n",
      "Collecting streamlit==0.82\n",
      "  Downloading streamlit-0.82.0-py2.py3-none-any.whl (8.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in ./mamba_env/lib/python3.10/site-packages (from promptsource==0.2.3) (3.1.3)\n",
      "Collecting plotly\n",
      "  Downloading plotly-5.18.0-py3-none-any.whl (15.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.6/15.6 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in ./mamba_env/lib/python3.10/site-packages (from promptsource==0.2.3) (2.31.0)\n",
      "Requirement already satisfied: pandas in ./mamba_env/lib/python3.10/site-packages (from promptsource==0.2.3) (2.1.4)\n",
      "Collecting py7zr\n",
      "  Downloading py7zr-0.20.8-py3-none-any.whl (67 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydeck>=0.1.dev5\n",
      "  Downloading pydeck-0.8.1b0-py2.py3-none-any.whl (4.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting base58\n",
      "  Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n",
      "Collecting validators\n",
      "  Downloading validators-0.22.0-py3-none-any.whl (26 kB)\n",
      "Collecting watchdog\n",
      "  Downloading watchdog-3.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in ./mamba_env/lib/python3.10/site-packages (from streamlit==0.82->promptsource==0.2.3) (23.2)\n",
      "Collecting gitpython\n",
      "  Downloading GitPython-3.1.41-py3-none-any.whl (196 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil in ./mamba_env/lib/python3.10/site-packages (from streamlit==0.82->promptsource==0.2.3) (2.8.2)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading pillow-10.2.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting click<8.0,>=7.0\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.8/82.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting blinker\n",
      "  Downloading blinker-1.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting astor\n",
      "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Collecting altair>=3.2.0\n",
      "  Downloading altair-5.2.0-py3-none-any.whl (996 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m996.9/996.9 kB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting protobuf!=3.11,>=3.6.0\n",
      "  Downloading protobuf-4.25.2-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting toml\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting tzlocal\n",
      "  Downloading tzlocal-5.2-py3-none-any.whl (17 kB)\n",
      "Collecting cachetools>=4.0\n",
      "  Downloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Requirement already satisfied: numpy in ./mamba_env/lib/python3.10/site-packages (from streamlit==0.82->promptsource==0.2.3) (1.26.3)\n",
      "Requirement already satisfied: tornado>=5.0 in ./mamba_env/lib/python3.10/site-packages (from streamlit==0.82->promptsource==0.2.3) (6.4)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.0 in ./mamba_env/lib/python3.10/site-packages (from black<=21.12b0->promptsource==0.2.3) (4.9.0)\n",
      "Requirement already satisfied: platformdirs>=2 in ./mamba_env/lib/python3.10/site-packages (from black<=21.12b0->promptsource==0.2.3) (4.1.0)\n",
      "Collecting mypy-extensions>=0.4.3\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Collecting pathspec<1,>=0.9.0\n",
      "  Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
      "Collecting tomli<2.0.0,>=0.2.6\n",
      "  Downloading tomli-1.2.3-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in ./mamba_env/lib/python3.10/site-packages (from datasets>=1.7.0->promptsource==0.2.3) (14.0.2)\n",
      "Requirement already satisfied: aiohttp in ./mamba_env/lib/python3.10/site-packages (from datasets>=1.7.0->promptsource==0.2.3) (3.9.1)\n",
      "Requirement already satisfied: xxhash in ./mamba_env/lib/python3.10/site-packages (from datasets>=1.7.0->promptsource==0.2.3) (3.4.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in ./mamba_env/lib/python3.10/site-packages (from datasets>=1.7.0->promptsource==0.2.3) (0.20.2)\n",
      "Requirement already satisfied: multiprocess in ./mamba_env/lib/python3.10/site-packages (from datasets>=1.7.0->promptsource==0.2.3) (0.70.15)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in ./mamba_env/lib/python3.10/site-packages (from datasets>=1.7.0->promptsource==0.2.3) (4.66.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in ./mamba_env/lib/python3.10/site-packages (from datasets>=1.7.0->promptsource==0.2.3) (0.3.7)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in ./mamba_env/lib/python3.10/site-packages (from datasets>=1.7.0->promptsource==0.2.3) (2023.9.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in ./mamba_env/lib/python3.10/site-packages (from datasets>=1.7.0->promptsource==0.2.3) (0.6)\n",
      "Requirement already satisfied: filelock in ./mamba_env/lib/python3.10/site-packages (from datasets>=1.7.0->promptsource==0.2.3) (3.13.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./mamba_env/lib/python3.10/site-packages (from pandas->promptsource==0.2.3) (2023.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./mamba_env/lib/python3.10/site-packages (from pandas->promptsource==0.2.3) (2023.3.post1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./mamba_env/lib/python3.10/site-packages (from requests->promptsource==0.2.3) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./mamba_env/lib/python3.10/site-packages (from requests->promptsource==0.2.3) (2023.11.17)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./mamba_env/lib/python3.10/site-packages (from requests->promptsource==0.2.3) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./mamba_env/lib/python3.10/site-packages (from requests->promptsource==0.2.3) (3.3.2)\n",
      "Collecting pycodestyle<2.12.0,>=2.11.0\n",
      "  Downloading pycodestyle-2.11.1-py2.py3-none-any.whl (31 kB)\n",
      "Collecting mccabe<0.8.0,>=0.7.0\n",
      "  Downloading mccabe-0.7.0-py2.py3-none-any.whl (7.3 kB)\n",
      "Collecting pyflakes<3.3.0,>=3.2.0\n",
      "  Downloading pyflakes-3.2.0-py2.py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in ./mamba_env/lib/python3.10/site-packages (from jinja2->promptsource==0.2.3) (2.1.3)\n",
      "Collecting tenacity>=6.2.0\n",
      "  Downloading tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Collecting pybcj<1.1.0,>=1.0.0\n",
      "  Downloading pybcj-1.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyzstd>=0.15.9\n",
      "  Downloading pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (412 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.3/412.3 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting inflate64<1.1.0,>=1.0.0\n",
      "  Downloading inflate64-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (93 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting brotli>=1.1.0\n",
      "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyppmd<1.2.0,>=1.1.0\n",
      "  Downloading pyppmd-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.9/138.9 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting multivolumefile>=0.2.3\n",
      "  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
      "Collecting pycryptodomex>=3.16.0\n",
      "  Downloading pycryptodomex-3.20.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting texttable\n",
      "  Downloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: psutil in ./mamba_env/lib/python3.10/site-packages (from py7zr->promptsource==0.2.3) (5.9.7)\n",
      "Collecting pluggy<2.0,>=0.12\n",
      "  Downloading pluggy-1.3.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in ./mamba_env/lib/python3.10/site-packages (from pytest->promptsource==0.2.3) (1.2.0)\n",
      "Collecting iniconfig\n",
      "  Downloading iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\n",
      "Collecting toolz\n",
      "  Downloading toolz-0.12.0-py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting jsonschema>=3.0\n",
      "  Downloading jsonschema-4.20.0-py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.7/84.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: yarl<2.0,>=1.0 in ./mamba_env/lib/python3.10/site-packages (from aiohttp->datasets>=1.7.0->promptsource==0.2.3) (1.9.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./mamba_env/lib/python3.10/site-packages (from aiohttp->datasets>=1.7.0->promptsource==0.2.3) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./mamba_env/lib/python3.10/site-packages (from aiohttp->datasets>=1.7.0->promptsource==0.2.3) (6.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./mamba_env/lib/python3.10/site-packages (from aiohttp->datasets>=1.7.0->promptsource==0.2.3) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./mamba_env/lib/python3.10/site-packages (from aiohttp->datasets>=1.7.0->promptsource==0.2.3) (1.4.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in ./mamba_env/lib/python3.10/site-packages (from aiohttp->datasets>=1.7.0->promptsource==0.2.3) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in ./mamba_env/lib/python3.10/site-packages (from python-dateutil->streamlit==0.82->promptsource==0.2.3) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Collecting referencing>=0.28.4\n",
      "  Downloading referencing-0.32.1-py3-none-any.whl (26 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6\n",
      "  Downloading jsonschema_specifications-2023.12.1-py3-none-any.whl (18 kB)\n",
      "Collecting rpds-py>=0.7.1\n",
      "  Downloading rpds_py-0.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: promptsource\n",
      "  Building wheel for promptsource (setup.py): started\n",
      "  Building wheel for promptsource (setup.py): finished with status 'done'\n",
      "  Created wheel for promptsource: filename=promptsource-0.2.3-py3-none-any.whl size=381651 sha256=692353a6b126db4883b2752d7bea94df6905bb89515a9d6d028a8033ea72889b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-loltcd3s/wheels/cf/7f/d7/7a041682fc0da892182708365e803c643b6ca062908f9aeb7e\n",
      "Successfully built promptsource\n",
      "Installing collected packages: texttable, brotli, watchdog, validators, tzlocal, toolz, tomli, toml, tenacity, smmap, rpds-py, pyzstd, pyppmd, pyflakes, pycryptodomex, pycodestyle, pybcj, protobuf, pluggy, pillow, pathspec, mypy-extensions, multivolumefile, mccabe, isort, iniconfig, inflate64, click, cachetools, blinker, base58, astor, referencing, pytest, pydeck, py7zr, plotly, gitdb, flake8, black, jsonschema-specifications, gitpython, jsonschema, altair, streamlit, promptsource\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.1.7\n",
      "    Uninstalling click-8.1.7:\n",
      "      Successfully uninstalled click-8.1.7\n",
      "Successfully installed altair-5.2.0 astor-0.8.1 base58-2.1.1 black-21.12b0 blinker-1.7.0 brotli-1.1.0 cachetools-5.3.2 click-7.1.2 flake8-7.0.0 gitdb-4.0.11 gitpython-3.1.41 inflate64-1.0.0 iniconfig-2.0.0 isort-5.8.0 jsonschema-4.20.0 jsonschema-specifications-2023.12.1 mccabe-0.7.0 multivolumefile-0.2.3 mypy-extensions-1.0.0 pathspec-0.12.1 pillow-10.2.0 plotly-5.18.0 pluggy-1.3.0 promptsource-0.2.3 protobuf-4.25.2 py7zr-0.20.8 pybcj-1.0.2 pycodestyle-2.11.1 pycryptodomex-3.20.0 pydeck-0.8.1b0 pyflakes-3.2.0 pyppmd-1.1.0 pytest-7.4.4 pyzstd-0.15.9 referencing-0.32.1 rpds-py-0.17.1 smmap-5.0.1 streamlit-0.82.0 tenacity-8.2.3 texttable-1.7.0 toml-0.10.2 tomli-1.2.3 toolz-0.12.0 tzlocal-5.2 validators-0.22.0 watchdog-3.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flash_attn\n",
      "  Downloading flash_attn-2.4.2.tar.gz (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25h  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: torch in ./mamba_env/lib/python3.10/site-packages (from flash_attn) (2.1.2)\n",
      "Requirement already satisfied: einops in ./mamba_env/lib/python3.10/site-packages (from flash_attn) (0.7.0)\n",
      "Requirement already satisfied: packaging in ./mamba_env/lib/python3.10/site-packages (from flash_attn) (23.2)\n",
      "Requirement already satisfied: ninja in ./mamba_env/lib/python3.10/site-packages (from flash_attn) (1.11.1.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in ./mamba_env/lib/python3.10/site-packages (from torch->flash_attn) (2.18.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./mamba_env/lib/python3.10/site-packages (from torch->flash_attn) (11.0.2.54)\n",
      "Requirement already satisfied: typing-extensions in ./mamba_env/lib/python3.10/site-packages (from torch->flash_attn) (4.9.0)\n",
      "Requirement already satisfied: triton==2.1.0 in ./mamba_env/lib/python3.10/site-packages (from torch->flash_attn) (2.1.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./mamba_env/lib/python3.10/site-packages (from torch->flash_attn) (12.1.105)\n",
      "Requirement already satisfied: filelock in ./mamba_env/lib/python3.10/site-packages (from torch->flash_attn) (3.13.1)\n",
      "Requirement already satisfied: jinja2 in ./mamba_env/lib/python3.10/site-packages (from torch->flash_attn) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./mamba_env/lib/python3.10/site-packages (from torch->flash_attn) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./mamba_env/lib/python3.10/site-packages (from torch->flash_attn) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./mamba_env/lib/python3.10/site-packages (from torch->flash_attn) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./mamba_env/lib/python3.10/site-packages (from torch->flash_attn) (12.1.105)\n",
      "Requirement already satisfied: sympy in ./mamba_env/lib/python3.10/site-packages (from torch->flash_attn) (1.12)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./mamba_env/lib/python3.10/site-packages (from torch->flash_attn) (12.1.3.1)\n",
      "Requirement already satisfied: networkx in ./mamba_env/lib/python3.10/site-packages (from torch->flash_attn) (3.2.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./mamba_env/lib/python3.10/site-packages (from torch->flash_attn) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./mamba_env/lib/python3.10/site-packages (from torch->flash_attn) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./mamba_env/lib/python3.10/site-packages (from torch->flash_attn) (12.1.105)\n",
      "Requirement already satisfied: fsspec in ./mamba_env/lib/python3.10/site-packages (from torch->flash_attn) (2023.9.2)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./mamba_env/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash_attn) (12.3.101)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./mamba_env/lib/python3.10/site-packages (from jinja2->torch->flash_attn) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./mamba_env/lib/python3.10/site-packages (from sympy->torch->flash_attn) (1.3.0)\n",
      "Building wheels for collected packages: flash_attn\n",
      "  Building wheel for flash_attn (setup.py): started\n",
      "  Building wheel for flash_attn (setup.py): finished with status 'done'\n",
      "  Created wheel for flash_attn: filename=flash_attn-2.4.2-cp310-cp310-linux_x86_64.whl size=113714653 sha256=560df8869000f20f44af8ff42721c3d08ccfe5624bdc3c00e056a783e2a54fb9\n",
      "  Stored in directory: /home/.cache/pip/wheels/9d/cf/7f/d14555553b5b30698dae0a4159fdd058157e7021cec565ecaa\n",
      "Successfully built flash_attn\n",
      "Installing collected packages: flash_attn\n",
      "Successfully installed flash_attn-2.4.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed kernelspec mamba_env in /home/.local/share/jupyter/kernels/mamba_env\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python -m venv mamba_env\n",
    "source mamba_env/bin/activate\n",
    "\n",
    "#need this for lambda stack\n",
    "#for cudnn_so in /usr/lib/python3/dist-packages/tensorflow/libcudnn*; do\n",
    "#  sudo ln -s \"$cudnn_so\" /usr/lib/x86_64-linux-gnu/\n",
    "#done\n",
    "\n",
    "pip install packaging wheel torch ipykernel datasets\n",
    "pip install accelerate -U\n",
    "#assume your in mamba directory\n",
    "pip install .\n",
    "\n",
    "#install bigcode eval harness\n",
    "cd 3rdparty\n",
    "git clone https://github.com/bigcode-project/bigcode-evaluation-harness.git\n",
    "cd ..\n",
    "pip install -e 3rdparty/bigcode-evaluation-harness\n",
    "\n",
    "#install lm eleuther ai eval harness\n",
    "git submodule update --init --recursive\n",
    "pip install -e 3rdparty/lm-evaluation-harness\n",
    "pip install git+https://github.com/bigscience-workshop/promptsource.git\n",
    "#install flash attention for phi model\n",
    "pip install flash_attn\n",
    "\n",
    "#install environment as a ipykernel for use in jupyter notebook\n",
    "python -m ipykernel install --user --name=mamba_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval on squad 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-12:22:20:15,536 INFO     [utils.py:148] Note: NumExpr detected 64 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2024-01-12:22:20:15,537 INFO     [utils.py:160] NumExpr defaulting to 8 threads.\n",
      "2024-01-12:22:20:16,420 INFO     [config.py:58] PyTorch version 2.1.2 available.\n",
      "Downloading builder script: 100%|██████████| 5.67k/5.67k [00:00<00:00, 7.04MB/s]\n",
      "2024-01-12:22:20:31,619 WARNING  [templates.py:592] Tried instantiating `DatasetTemplates` for gsmk boolq, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
      "2024-01-12:22:20:31,622 WARNING  [templates.py:592] Tried instantiating `DatasetTemplates` for EleutherAI/asdiv, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
      "2024-01-12:22:20:35,679 INFO     [__main__.py:184] Selected Tasks: ['lambada_openai']\n",
      "config.json: 100%|██████████| 199/199 [00:00<00:00, 401kB/s]\n",
      "pytorch_model.bin: 100%|██████████| 517M/517M [00:06<00:00, 85.2MB/s] \n",
      "tokenizer_config.json: 100%|██████████| 156/156 [00:00<00:00, 355kB/s]\n",
      "vocab.json: 100%|██████████| 1.08M/1.08M [00:00<00:00, 1.24MB/s]\n",
      "merges.txt: 100%|██████████| 457k/457k [00:00<00:00, 706kB/s]\n",
      "tokenizer.json: 100%|██████████| 2.11M/2.11M [00:01<00:00, 1.98MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 166kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Downloading data: 100%|██████████| 1.16M/1.16M [00:02<00:00, 506kB/s]\n",
      "Generating test split: 100%|██████████| 5153/5153 [00:00<00:00, 113310.24 examples/s]\n",
      "2024-01-12:22:20:59,599 WARNING  [task.py:303] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2024-01-12:22:20:59,599 WARNING  [task.py:303] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2024-01-12:22:20:59,705 INFO     [task.py:358] Building contexts for task on rank 0...\n",
      "2024-01-12:22:21:10,130 INFO     [evaluator.py:256] Task: lambada_openai; number of requests on this rank: 5153\n",
      "2024-01-12:22:21:10,132 INFO     [evaluator.py:288] Running loglikelihood requests\n",
      "100%|██████████| 5153/5153 [00:15<00:00, 336.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bootstrapping for stddev: perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:10<00:00,  9.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mamba (pretrained=state-spaces/mamba-130m), limit: None, num_fewshot: None, batch_size: 64\n",
      "|    Tasks     |Version|Filter|  Metric  | Value |   |Stderr|\n",
      "|--------------|-------|------|----------|------:|---|-----:|\n",
      "|lambada_openai|Yaml   |none  |perplexity|16.0459|±  |0.5093|\n",
      "|              |       |none  |acc       | 0.4425|±  |0.0069|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source mamba_env/bin/activate\n",
    "python evals/lm_harness_eval.py --model mamba \\\n",
    "    --model_args pretrained=state-spaces/mamba-130m \\\n",
    "    --tasks lambada_openai --device cuda:0 --batch_size 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-12:22:22:27,974 INFO     [utils.py:148] Note: NumExpr detected 64 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2024-01-12:22:22:27,975 INFO     [utils.py:160] NumExpr defaulting to 8 threads.\n",
      "2024-01-12:22:22:28,790 INFO     [config.py:58] PyTorch version 2.1.2 available.\n",
      "2024-01-12:22:22:39,225 WARNING  [templates.py:592] Tried instantiating `DatasetTemplates` for gsmk boolq, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
      "2024-01-12:22:22:39,227 WARNING  [templates.py:592] Tried instantiating `DatasetTemplates` for EleutherAI/asdiv, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
      "2024-01-12:22:22:42,449 INFO     [__main__.py:184] Selected Tasks: ['lambada_openai']\n",
      "2024-01-12:22:22:42,466 INFO     [huggingface.py:118] Using device 'cuda'\n",
      "config.json: 100%|██████████| 570/570 [00:00<00:00, 1.18MB/s]\n",
      "pytorch_model.bin: 100%|██████████| 911M/911M [00:44<00:00, 20.6MB/s] \n",
      "tokenizer_config.json: 100%|██████████| 396/396 [00:00<00:00, 903kB/s]\n",
      "tokenizer.json: 100%|██████████| 2.11M/2.11M [00:01<00:00, 1.93MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 99.0/99.0 [00:00<00:00, 264kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2024-01-12:22:23:44,208 WARNING  [task.py:303] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2024-01-12:22:23:44,208 WARNING  [task.py:303] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2024-01-12:22:23:44,353 INFO     [task.py:358] Building contexts for task on rank 0...\n",
      "2024-01-12:22:23:54,075 INFO     [evaluator.py:256] Task: lambada_openai; number of requests on this rank: 5153\n",
      "2024-01-12:22:23:54,077 INFO     [evaluator.py:288] Running loglikelihood requests\n",
      "100%|██████████| 5153/5153 [00:38<00:00, 133.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bootstrapping for stddev: perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:09<00:00, 10.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf (pretrained=EleutherAI/pythia-410m,revision=step100000,dtype=float), limit: None, num_fewshot: None, batch_size: 64\n",
      "|    Tasks     |Version|Filter|  Metric  | Value |   |Stderr|\n",
      "|--------------|-------|------|----------|------:|---|-----:|\n",
      "|lambada_openai|Yaml   |none  |perplexity|11.1080|±  |0.3335|\n",
      "|              |       |none  |acc       | 0.5115|±  |0.0070|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source mamba_env/bin/activate\n",
    "python evals/lm_harness_eval.py --model hf \\\n",
    "    --model_args pretrained=EleutherAI/pythia-410m,revision=step100000,dtype=\"float\" \\\n",
    "    --tasks lambada_openai --device cuda --batch_size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-12:22:38:55,870 INFO     [utils.py:148] Note: NumExpr detected 64 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "2024-01-12:22:38:55,870 INFO     [utils.py:160] NumExpr defaulting to 8 threads.\n",
      "2024-01-12:22:38:56,255 INFO     [config.py:58] PyTorch version 2.1.2 available.\n",
      "2024-01-12:22:39:04,415 WARNING  [templates.py:592] Tried instantiating `DatasetTemplates` for gsmk boolq, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
      "2024-01-12:22:39:04,417 WARNING  [templates.py:592] Tried instantiating `DatasetTemplates` for EleutherAI/asdiv, but no prompts found. Please ignore this warning if you are creating new prompts for this dataset.\n",
      "2024-01-12:22:39:07,516 INFO     [__main__.py:184] Selected Tasks: ['lambada_openai']\n",
      "2024-01-12:22:39:07,530 INFO     [huggingface.py:118] Using device 'cuda'\n",
      "model.safetensors.index.json: 100%|██████████| 35.7k/35.7k [00:00<00:00, 169kB/s]\n",
      "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   0%|          | 10.5M/5.00G [00:00<01:29, 55.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|          | 31.5M/5.00G [00:00<00:57, 86.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|          | 52.4M/5.00G [00:00<00:50, 97.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|▏         | 62.9M/5.00G [00:00<00:49, 99.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   2%|▏         | 83.9M/5.00G [00:00<00:48, 102MB/s] \u001b[A\n",
      "model-00001-of-00002.safetensors:   2%|▏         | 94.4M/5.00G [00:00<00:47, 102MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   2%|▏         | 105M/5.00G [00:01<00:47, 103MB/s] \u001b[A\n",
      "model-00001-of-00002.safetensors:   2%|▏         | 115M/5.00G [00:01<00:47, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|▎         | 126M/5.00G [00:01<00:47, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|▎         | 147M/5.00G [00:01<00:46, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|▎         | 157M/5.00G [00:01<00:46, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|▎         | 168M/5.00G [00:01<00:47, 102MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4%|▎         | 178M/5.00G [00:01<00:46, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4%|▍         | 199M/5.00G [00:01<00:45, 106MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4%|▍         | 210M/5.00G [00:02<00:45, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|▍         | 231M/5.00G [00:02<00:45, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|▍         | 241M/5.00G [00:02<00:45, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|▌         | 252M/5.00G [00:02<00:45, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|▌         | 273M/5.00G [00:02<00:45, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   6%|▌         | 294M/5.00G [00:02<00:44, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   6%|▌         | 304M/5.00G [00:02<00:45, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|▋         | 325M/5.00G [00:03<00:45, 102MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|▋         | 336M/5.00G [00:03<00:45, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|▋         | 346M/5.00G [00:03<00:45, 102MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|▋         | 367M/5.00G [00:03<00:44, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|▊         | 377M/5.00G [00:03<00:44, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|▊         | 388M/5.00G [00:03<00:44, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|▊         | 398M/5.00G [00:03<00:44, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|▊         | 409M/5.00G [00:04<00:45, 101MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|▊         | 419M/5.00G [00:04<00:45, 101MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|▊         | 430M/5.00G [00:04<00:44, 102MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|▉         | 440M/5.00G [00:04<00:44, 101MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|▉         | 451M/5.00G [00:04<00:44, 102MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|▉         | 461M/5.00G [00:04<00:44, 102MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|▉         | 472M/5.00G [00:04<00:44, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10%|▉         | 482M/5.00G [00:04<00:43, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10%|▉         | 493M/5.00G [00:04<00:43, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10%|█         | 503M/5.00G [00:04<00:43, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10%|█         | 524M/5.00G [00:05<00:42, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|█         | 535M/5.00G [00:05<00:42, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|█         | 545M/5.00G [00:05<00:42, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|█         | 556M/5.00G [00:05<00:42, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|█▏        | 566M/5.00G [00:05<00:42, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|█▏        | 577M/5.00G [00:05<00:42, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|█▏        | 587M/5.00G [00:05<00:42, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|█▏        | 598M/5.00G [00:05<00:43, 100MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|█▏        | 619M/5.00G [00:06<00:42, 102MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|█▎        | 640M/5.00G [00:06<00:42, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|█▎        | 661M/5.00G [00:06<00:41, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|█▎        | 671M/5.00G [00:06<00:41, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14%|█▍        | 692M/5.00G [00:06<00:41, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14%|█▍        | 713M/5.00G [00:06<00:40, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14%|█▍        | 724M/5.00G [00:07<00:40, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15%|█▍        | 744M/5.00G [00:07<00:40, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15%|█▌        | 765M/5.00G [00:07<00:40, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|█▌        | 786M/5.00G [00:07<00:40, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|█▌        | 797M/5.00G [00:07<00:40, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|█▋        | 818M/5.00G [00:07<00:39, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  17%|█▋        | 839M/5.00G [00:08<00:39, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  17%|█▋        | 860M/5.00G [00:08<00:39, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|█▊        | 881M/5.00G [00:08<00:39, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|█▊        | 902M/5.00G [00:08<00:39, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|█▊        | 912M/5.00G [00:08<00:38, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19%|█▊        | 933M/5.00G [00:09<00:38, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19%|█▉        | 954M/5.00G [00:09<00:38, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|█▉        | 975M/5.00G [00:09<00:38, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|█▉        | 996M/5.00G [00:09<00:37, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|██        | 1.02G/5.00G [00:09<00:37, 106MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  21%|██        | 1.04G/5.00G [00:10<00:37, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  21%|██        | 1.06G/5.00G [00:10<00:37, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|██▏       | 1.08G/5.00G [00:10<00:37, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|██▏       | 1.09G/5.00G [00:10<00:37, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|██▏       | 1.11G/5.00G [00:10<00:37, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|██▏       | 1.12G/5.00G [00:10<00:37, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  23%|██▎       | 1.13G/5.00G [00:10<00:37, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  23%|██▎       | 1.14G/5.00G [00:11<00:37, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  23%|██▎       | 1.15G/5.00G [00:11<00:37, 102MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  23%|██▎       | 1.16G/5.00G [00:11<00:37, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24%|██▎       | 1.18G/5.00G [00:11<00:36, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24%|██▍       | 1.20G/5.00G [00:11<00:36, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24%|██▍       | 1.21G/5.00G [00:11<00:36, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24%|██▍       | 1.22G/5.00G [00:11<00:36, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  25%|██▍       | 1.23G/5.00G [00:11<00:36, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  25%|██▍       | 1.24G/5.00G [00:11<00:36, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  25%|██▌       | 1.26G/5.00G [00:12<00:35, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26%|██▌       | 1.28G/5.00G [00:12<00:35, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26%|██▌       | 1.29G/5.00G [00:12<00:35, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26%|██▌       | 1.30G/5.00G [00:12<00:35, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26%|██▋       | 1.32G/5.00G [00:12<00:35, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27%|██▋       | 1.34G/5.00G [00:12<00:34, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27%|██▋       | 1.35G/5.00G [00:13<00:34, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27%|██▋       | 1.36G/5.00G [00:13<00:35, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28%|██▊       | 1.38G/5.00G [00:13<00:34, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28%|██▊       | 1.39G/5.00G [00:13<00:34, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28%|██▊       | 1.41G/5.00G [00:13<00:34, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28%|██▊       | 1.42G/5.00G [00:13<00:34, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29%|██▊       | 1.43G/5.00G [00:13<00:34, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29%|██▉       | 1.45G/5.00G [00:13<00:33, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29%|██▉       | 1.46G/5.00G [00:14<00:33, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29%|██▉       | 1.47G/5.00G [00:14<00:34, 102MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30%|██▉       | 1.49G/5.00G [00:14<00:33, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30%|███       | 1.51G/5.00G [00:14<00:33, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31%|███       | 1.53G/5.00G [00:14<00:33, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31%|███       | 1.54G/5.00G [00:14<00:33, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31%|███       | 1.55G/5.00G [00:15<00:33, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31%|███▏      | 1.56G/5.00G [00:15<00:32, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31%|███▏      | 1.57G/5.00G [00:15<00:33, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  32%|███▏      | 1.58G/5.00G [00:15<00:33, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  32%|███▏      | 1.59G/5.00G [00:15<00:32, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  32%|███▏      | 1.60G/5.00G [00:15<00:32, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  32%|███▏      | 1.61G/5.00G [00:15<00:32, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  33%|███▎      | 1.64G/5.00G [00:15<00:32, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  33%|███▎      | 1.66G/5.00G [00:16<00:31, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  33%|███▎      | 1.67G/5.00G [00:16<00:31, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|███▎      | 1.68G/5.00G [00:16<00:31, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|███▍      | 1.69G/5.00G [00:16<00:31, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|███▍      | 1.70G/5.00G [00:16<00:31, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|███▍      | 1.71G/5.00G [00:16<00:31, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|███▍      | 1.72G/5.00G [00:16<00:32, 99.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  35%|███▍      | 1.73G/5.00G [00:16<00:35, 91.0MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  35%|███▍      | 1.74G/5.00G [00:16<00:34, 93.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  35%|███▌      | 1.76G/5.00G [00:17<00:30, 105MB/s] \u001b[A\n",
      "model-00001-of-00002.safetensors:  36%|███▌      | 1.78G/5.00G [00:17<00:29, 108MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  36%|███▌      | 1.80G/5.00G [00:17<00:29, 107MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  37%|███▋      | 1.82G/5.00G [00:17<00:29, 106MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  37%|███▋      | 1.85G/5.00G [00:17<00:29, 106MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  37%|███▋      | 1.87G/5.00G [00:18<00:29, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  38%|███▊      | 1.89G/5.00G [00:18<00:29, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  38%|███▊      | 1.90G/5.00G [00:18<00:29, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  38%|███▊      | 1.92G/5.00G [00:18<00:29, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39%|███▊      | 1.93G/5.00G [00:18<00:29, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39%|███▉      | 1.95G/5.00G [00:18<00:28, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39%|███▉      | 1.97G/5.00G [00:19<00:28, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40%|███▉      | 1.99G/5.00G [00:19<00:28, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40%|████      | 2.00G/5.00G [00:19<00:28, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40%|████      | 2.01G/5.00G [00:19<00:28, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  41%|████      | 2.02G/5.00G [00:19<00:42, 69.5MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  41%|████      | 2.04G/5.00G [00:19<00:35, 83.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  41%|████▏     | 2.07G/5.00G [00:20<00:30, 94.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42%|████▏     | 2.09G/5.00G [00:20<00:28, 103MB/s] \u001b[A\n",
      "model-00001-of-00002.safetensors:  42%|████▏     | 2.11G/5.00G [00:20<00:28, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43%|████▎     | 2.13G/5.00G [00:20<00:27, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43%|████▎     | 2.15G/5.00G [00:20<00:27, 102MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43%|████▎     | 2.17G/5.00G [00:21<00:27, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  44%|████▍     | 2.19G/5.00G [00:21<00:27, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  44%|████▍     | 2.21G/5.00G [00:21<00:26, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  44%|████▍     | 2.22G/5.00G [00:21<00:26, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  45%|████▍     | 2.23G/5.00G [00:21<00:26, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  45%|████▌     | 2.25G/5.00G [00:21<00:26, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  45%|████▌     | 2.26G/5.00G [00:21<00:26, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46%|████▌     | 2.28G/5.00G [00:22<00:26, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46%|████▌     | 2.30G/5.00G [00:22<00:26, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46%|████▋     | 2.32G/5.00G [00:22<00:25, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  47%|████▋     | 2.33G/5.00G [00:22<00:25, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  47%|████▋     | 2.35G/5.00G [00:22<00:25, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  47%|████▋     | 2.37G/5.00G [00:23<00:25, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  48%|████▊     | 2.38G/5.00G [00:23<00:25, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  48%|████▊     | 2.39G/5.00G [00:23<00:24, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  48%|████▊     | 2.41G/5.00G [00:23<00:24, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  48%|████▊     | 2.42G/5.00G [00:23<00:24, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49%|████▊     | 2.43G/5.00G [00:23<00:24, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49%|████▉     | 2.44G/5.00G [00:23<00:24, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49%|████▉     | 2.46G/5.00G [00:23<00:24, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50%|████▉     | 2.47G/5.00G [00:24<00:24, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50%|████▉     | 2.49G/5.00G [00:24<00:24, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50%|████▉     | 2.50G/5.00G [00:24<00:24, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50%|█████     | 2.51G/5.00G [00:24<00:24, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50%|█████     | 2.52G/5.00G [00:24<00:23, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  51%|█████     | 2.54G/5.00G [00:24<00:23, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  51%|█████     | 2.56G/5.00G [00:24<00:23, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  52%|█████▏    | 2.58G/5.00G [00:25<00:23, 102MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  52%|█████▏    | 2.59G/5.00G [00:25<00:23, 102MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  52%|█████▏    | 2.61G/5.00G [00:25<00:22, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  52%|█████▏    | 2.62G/5.00G [00:25<00:22, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%|█████▎    | 2.63G/5.00G [00:25<00:22, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%|█████▎    | 2.64G/5.00G [00:25<00:22, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%|█████▎    | 2.65G/5.00G [00:25<00:22, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%|█████▎    | 2.66G/5.00G [00:25<00:22, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  54%|█████▎    | 2.67G/5.00G [00:25<00:22, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  54%|█████▎    | 2.68G/5.00G [00:26<00:22, 102MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  54%|█████▍    | 2.71G/5.00G [00:26<00:22, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55%|█████▍    | 2.73G/5.00G [00:26<00:21, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55%|█████▍    | 2.74G/5.00G [00:26<00:21, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55%|█████▍    | 2.75G/5.00G [00:26<00:21, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55%|█████▌    | 2.77G/5.00G [00:26<00:21, 101MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  56%|█████▌    | 2.78G/5.00G [00:26<00:21, 101MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  56%|█████▌    | 2.80G/5.00G [00:27<00:20, 106MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  56%|█████▋    | 2.82G/5.00G [00:27<00:20, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  57%|█████▋    | 2.84G/5.00G [00:27<00:20, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  57%|█████▋    | 2.86G/5.00G [00:27<00:20, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  58%|█████▊    | 2.88G/5.00G [00:27<00:20, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  58%|█████▊    | 2.90G/5.00G [00:28<00:19, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  58%|█████▊    | 2.92G/5.00G [00:28<00:20, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59%|█████▊    | 2.93G/5.00G [00:28<00:20, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59%|█████▉    | 2.95G/5.00G [00:28<00:19, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59%|█████▉    | 2.96G/5.00G [00:28<00:19, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59%|█████▉    | 2.97G/5.00G [00:28<00:19, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60%|█████▉    | 2.99G/5.00G [00:28<00:19, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60%|██████    | 3.00G/5.00G [00:29<00:19, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60%|██████    | 3.02G/5.00G [00:29<00:18, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  61%|██████    | 3.04G/5.00G [00:29<00:18, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  61%|██████    | 3.05G/5.00G [00:29<00:18, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62%|██████▏   | 3.07G/5.00G [00:29<00:18, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62%|██████▏   | 3.09G/5.00G [00:29<00:18, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62%|██████▏   | 3.10G/5.00G [00:30<00:18, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62%|██████▏   | 3.11G/5.00G [00:30<00:18, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63%|██████▎   | 3.12G/5.00G [00:30<00:17, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63%|██████▎   | 3.14G/5.00G [00:30<00:17, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63%|██████▎   | 3.15G/5.00G [00:30<00:17, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63%|██████▎   | 3.16G/5.00G [00:30<00:17, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63%|██████▎   | 3.17G/5.00G [00:30<00:17, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64%|██████▍   | 3.19G/5.00G [00:30<00:17, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64%|██████▍   | 3.20G/5.00G [00:30<00:17, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64%|██████▍   | 3.22G/5.00G [00:31<00:16, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65%|██████▍   | 3.23G/5.00G [00:31<00:16, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65%|██████▍   | 3.24G/5.00G [00:31<00:16, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65%|██████▌   | 3.26G/5.00G [00:31<00:16, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65%|██████▌   | 3.27G/5.00G [00:31<00:16, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  66%|██████▌   | 3.29G/5.00G [00:31<00:16, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  66%|██████▌   | 3.30G/5.00G [00:31<00:16, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  66%|██████▋   | 3.31G/5.00G [00:32<00:16, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|██████▋   | 3.32G/5.00G [00:32<00:16, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|██████▋   | 3.33G/5.00G [00:32<00:16, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|██████▋   | 3.34G/5.00G [00:32<00:15, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|██████▋   | 3.36G/5.00G [00:32<00:15, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|██████▋   | 3.37G/5.00G [00:32<00:15, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68%|██████▊   | 3.38G/5.00G [00:32<00:15, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68%|██████▊   | 3.39G/5.00G [00:32<00:15, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68%|██████▊   | 3.40G/5.00G [00:32<00:15, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68%|██████▊   | 3.41G/5.00G [00:32<00:15, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68%|██████▊   | 3.42G/5.00G [00:33<00:15, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69%|██████▊   | 3.43G/5.00G [00:33<00:15, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69%|██████▉   | 3.44G/5.00G [00:33<00:15, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69%|██████▉   | 3.45G/5.00G [00:33<00:15, 101MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69%|██████▉   | 3.47G/5.00G [00:33<00:14, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  70%|██████▉   | 3.48G/5.00G [00:33<00:14, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  70%|██████▉   | 3.49G/5.00G [00:33<00:14, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  70%|███████   | 3.50G/5.00G [00:33<00:14, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71%|███████   | 3.52G/5.00G [00:34<00:14, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71%|███████   | 3.53G/5.00G [00:34<00:14, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71%|███████   | 3.54G/5.00G [00:34<00:14, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71%|███████   | 3.55G/5.00G [00:34<00:13, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71%|███████▏  | 3.57G/5.00G [00:34<00:13, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  72%|███████▏  | 3.59G/5.00G [00:34<00:13, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  72%|███████▏  | 3.61G/5.00G [00:34<00:13, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  72%|███████▏  | 3.62G/5.00G [00:35<00:13, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73%|███████▎  | 3.63G/5.00G [00:35<00:13, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73%|███████▎  | 3.64G/5.00G [00:35<00:13, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73%|███████▎  | 3.65G/5.00G [00:35<00:13, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73%|███████▎  | 3.66G/5.00G [00:35<00:13, 102MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73%|███████▎  | 3.67G/5.00G [00:35<00:13, 98.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  74%|███████▎  | 3.68G/5.00G [00:35<00:13, 98.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  74%|███████▍  | 3.69G/5.00G [00:35<00:13, 99.7MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  74%|███████▍  | 3.71G/5.00G [00:35<00:12, 101MB/s] \u001b[A\n",
      "model-00001-of-00002.safetensors:  75%|███████▍  | 3.72G/5.00G [00:36<00:12, 102MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75%|███████▍  | 3.73G/5.00G [00:36<00:12, 102MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75%|███████▍  | 3.74G/5.00G [00:36<00:12, 101MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75%|███████▌  | 3.75G/5.00G [00:36<00:12, 102MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75%|███████▌  | 3.76G/5.00G [00:36<00:12, 102MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76%|███████▌  | 3.77G/5.00G [00:36<00:11, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76%|███████▌  | 3.79G/5.00G [00:36<00:11, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76%|███████▌  | 3.80G/5.00G [00:36<00:11, 101MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76%|███████▌  | 3.81G/5.00G [00:36<00:11, 101MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76%|███████▋  | 3.82G/5.00G [00:36<00:11, 102MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77%|███████▋  | 3.83G/5.00G [00:37<00:11, 102MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77%|███████▋  | 3.84G/5.00G [00:37<00:11, 102MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77%|███████▋  | 3.86G/5.00G [00:37<00:10, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77%|███████▋  | 3.87G/5.00G [00:37<00:10, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78%|███████▊  | 3.89G/5.00G [00:37<00:10, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78%|███████▊  | 3.90G/5.00G [00:37<00:10, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78%|███████▊  | 3.91G/5.00G [00:37<00:10, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79%|███████▊  | 3.93G/5.00G [00:38<00:10, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79%|███████▉  | 3.94G/5.00G [00:38<00:10, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79%|███████▉  | 3.95G/5.00G [00:38<00:10, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79%|███████▉  | 3.96G/5.00G [00:38<00:10, 102MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|███████▉  | 3.97G/5.00G [00:38<00:09, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|███████▉  | 3.98G/5.00G [00:38<00:09, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|███████▉  | 4.00G/5.00G [00:38<00:09, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|████████  | 4.01G/5.00G [00:38<00:09, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|████████  | 4.02G/5.00G [00:38<00:09, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81%|████████  | 4.04G/5.00G [00:39<00:09, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81%|████████  | 4.05G/5.00G [00:39<00:09, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81%|████████▏ | 4.07G/5.00G [00:39<00:08, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82%|████████▏ | 4.08G/5.00G [00:39<00:08, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82%|████████▏ | 4.10G/5.00G [00:39<00:08, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82%|████████▏ | 4.12G/5.00G [00:39<00:08, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83%|████████▎ | 4.13G/5.00G [00:40<00:08, 101MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83%|████████▎ | 4.15G/5.00G [00:40<00:07, 106MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83%|████████▎ | 4.16G/5.00G [00:40<00:11, 74.2MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84%|████████▍ | 4.18G/5.00G [00:40<00:09, 87.1MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84%|████████▍ | 4.20G/5.00G [00:40<00:08, 97.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  85%|████████▍ | 4.23G/5.00G [00:41<00:07, 99.4MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  85%|████████▍ | 4.24G/5.00G [00:41<00:07, 100MB/s] \u001b[A\n",
      "model-00001-of-00002.safetensors:  85%|████████▌ | 4.25G/5.00G [00:41<00:07, 101MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  85%|████████▌ | 4.26G/5.00G [00:41<00:07, 101MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  85%|████████▌ | 4.27G/5.00G [00:41<00:07, 101MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86%|████████▌ | 4.28G/5.00G [00:41<00:07, 101MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86%|████████▌ | 4.29G/5.00G [00:41<00:07, 101MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86%|████████▌ | 4.30G/5.00G [00:41<00:06, 101MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86%|████████▋ | 4.31G/5.00G [00:41<00:06, 102MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86%|████████▋ | 4.32G/5.00G [00:41<00:06, 102MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87%|████████▋ | 4.33G/5.00G [00:42<00:06, 102MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87%|████████▋ | 4.34G/5.00G [00:42<00:06, 102MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87%|████████▋ | 4.35G/5.00G [00:42<00:06, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87%|████████▋ | 4.36G/5.00G [00:42<00:06, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88%|████████▊ | 4.37G/5.00G [00:42<00:06, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88%|████████▊ | 4.38G/5.00G [00:42<00:05, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88%|████████▊ | 4.39G/5.00G [00:42<00:05, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88%|████████▊ | 4.40G/5.00G [00:42<00:05, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88%|████████▊ | 4.41G/5.00G [00:42<00:05, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  89%|████████▊ | 4.42G/5.00G [00:42<00:05, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  89%|████████▉ | 4.45G/5.00G [00:43<00:05, 102MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  89%|████████▉ | 4.46G/5.00G [00:43<00:05, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90%|████████▉ | 4.48G/5.00G [00:43<00:04, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90%|████████▉ | 4.49G/5.00G [00:43<00:04, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90%|█████████ | 4.50G/5.00G [00:43<00:04, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90%|█████████ | 4.52G/5.00G [00:43<00:04, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91%|█████████ | 4.53G/5.00G [00:43<00:04, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91%|█████████ | 4.55G/5.00G [00:44<00:04, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91%|█████████▏| 4.56G/5.00G [00:44<00:04, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|█████████▏| 4.57G/5.00G [00:44<00:04, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|█████████▏| 4.58G/5.00G [00:44<00:04, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|█████████▏| 4.59G/5.00G [00:44<00:03, 102MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|█████████▏| 4.60G/5.00G [00:44<00:03, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|█████████▏| 4.61G/5.00G [00:44<00:03, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  93%|█████████▎| 4.62G/5.00G [00:44<00:03, 99.6MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  93%|█████████▎| 4.63G/5.00G [00:45<00:03, 100MB/s] \u001b[A\n",
      "model-00001-of-00002.safetensors:  93%|█████████▎| 4.66G/5.00G [00:45<00:03, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  93%|█████████▎| 4.67G/5.00G [00:45<00:03, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94%|█████████▍| 4.69G/5.00G [00:45<00:02, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94%|█████████▍| 4.70G/5.00G [00:45<00:02, 102MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94%|█████████▍| 4.71G/5.00G [00:45<00:02, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94%|█████████▍| 4.72G/5.00G [00:45<00:02, 102MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95%|█████████▍| 4.73G/5.00G [00:45<00:02, 102MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95%|█████████▍| 4.74G/5.00G [00:46<00:02, 102MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95%|█████████▌| 4.76G/5.00G [00:46<00:02, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96%|█████████▌| 4.77G/5.00G [00:46<00:02, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96%|█████████▌| 4.78G/5.00G [00:46<00:02, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96%|█████████▌| 4.79G/5.00G [00:46<00:01, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96%|█████████▋| 4.81G/5.00G [00:46<00:01, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97%|█████████▋| 4.82G/5.00G [00:46<00:01, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97%|█████████▋| 4.84G/5.00G [00:47<00:01, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97%|█████████▋| 4.85G/5.00G [00:47<00:01, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97%|█████████▋| 4.87G/5.00G [00:47<00:01, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98%|█████████▊| 4.88G/5.00G [00:47<00:01, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98%|█████████▊| 4.89G/5.00G [00:47<00:01, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98%|█████████▊| 4.90G/5.00G [00:47<00:00, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98%|█████████▊| 4.92G/5.00G [00:47<00:00, 105MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99%|█████████▊| 4.93G/5.00G [00:47<00:00, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99%|█████████▉| 4.94G/5.00G [00:47<00:00, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99%|█████████▉| 4.95G/5.00G [00:48<00:00, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99%|█████████▉| 4.96G/5.00G [00:48<00:00, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99%|█████████▉| 4.97G/5.00G [00:48<00:00, 104MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors: 100%|█████████▉| 4.98G/5.00G [00:48<00:00, 103MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors: 100%|██████████| 5.00G/5.00G [00:48<00:00, 103MB/s]\u001b[A\n",
      "Downloading shards:  50%|█████     | 1/2 [00:55<00:55, 55.12s/it]\n",
      "model-00002-of-00002.safetensors:   0%|          | 0.00/564M [00:00<?, ?B/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   2%|▏         | 10.5M/564M [00:00<00:16, 33.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   6%|▌         | 31.5M/564M [00:00<00:07, 70.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   9%|▉         | 52.4M/564M [00:00<00:06, 83.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  11%|█         | 62.9M/564M [00:00<00:05, 84.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  13%|█▎        | 73.4M/564M [00:00<00:05, 84.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  15%|█▍        | 83.9M/564M [00:01<00:05, 82.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  17%|█▋        | 94.4M/564M [00:01<00:05, 86.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  19%|█▊        | 105M/564M [00:01<00:05, 86.4MB/s] \u001b[A\n",
      "model-00002-of-00002.safetensors:  20%|██        | 115M/564M [00:01<00:05, 86.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  22%|██▏       | 126M/564M [00:01<00:05, 86.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  24%|██▍       | 136M/564M [00:01<00:04, 85.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  26%|██▌       | 147M/564M [00:01<00:04, 86.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  28%|██▊       | 157M/564M [00:01<00:04, 86.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  30%|██▉       | 168M/564M [00:02<00:04, 86.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  32%|███▏      | 178M/564M [00:02<00:04, 86.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  33%|███▎      | 189M/564M [00:02<00:04, 86.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  35%|███▌      | 199M/564M [00:02<00:04, 87.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  37%|███▋      | 210M/564M [00:02<00:04, 86.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  39%|███▉      | 220M/564M [00:02<00:03, 86.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  41%|████      | 231M/564M [00:02<00:03, 86.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  43%|████▎     | 241M/564M [00:02<00:03, 85.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  45%|████▍     | 252M/564M [00:03<00:03, 85.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  46%|████▋     | 262M/564M [00:03<00:03, 85.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  48%|████▊     | 273M/564M [00:03<00:03, 85.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  50%|█████     | 283M/564M [00:03<00:03, 86.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  52%|█████▏    | 294M/564M [00:03<00:03, 85.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  54%|█████▍    | 304M/564M [00:03<00:03, 85.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  56%|█████▌    | 315M/564M [00:03<00:02, 84.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  58%|█████▊    | 325M/564M [00:03<00:02, 84.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  60%|█████▉    | 336M/564M [00:03<00:02, 84.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  61%|██████▏   | 346M/564M [00:04<00:02, 85.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  63%|██████▎   | 357M/564M [00:04<00:02, 85.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  65%|██████▌   | 367M/564M [00:04<00:02, 85.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  67%|██████▋   | 377M/564M [00:04<00:02, 84.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  69%|██████▉   | 388M/564M [00:04<00:02, 85.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  71%|███████   | 398M/564M [00:04<00:01, 84.5MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  73%|███████▎  | 409M/564M [00:04<00:01, 84.8MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  74%|███████▍  | 419M/564M [00:04<00:01, 85.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  76%|███████▌  | 430M/564M [00:05<00:01, 83.9MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  78%|███████▊  | 440M/564M [00:05<00:01, 85.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  80%|███████▉  | 451M/564M [00:05<00:01, 85.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  82%|████████▏ | 461M/564M [00:05<00:01, 85.2MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  84%|████████▎ | 472M/564M [00:05<00:01, 85.3MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  86%|████████▌ | 482M/564M [00:05<00:00, 84.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  87%|████████▋ | 493M/564M [00:05<00:00, 84.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  89%|████████▉ | 503M/564M [00:05<00:00, 85.7MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  91%|█████████ | 514M/564M [00:06<00:00, 85.6MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  93%|█████████▎| 524M/564M [00:06<00:00, 86.1MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  95%|█████████▍| 535M/564M [00:06<00:00, 86.0MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  97%|█████████▋| 545M/564M [00:06<00:00, 85.4MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors: 100%|██████████| 564M/564M [00:06<00:00, 84.3MB/s]\u001b[A\n",
      "Downloading shards: 100%|██████████| 2/2 [01:03<00:00, 31.79s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.83it/s]\n",
      "generation_config.json: 100%|██████████| 74.0/74.0 [00:00<00:00, 192kB/s]\n",
      "tokenizer_config.json: 100%|██████████| 7.34k/7.34k [00:00<00:00, 14.8MB/s]\n",
      "vocab.json: 100%|██████████| 798k/798k [00:00<00:00, 926kB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.05MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.11M/2.11M [00:01<00:00, 1.92MB/s]\n",
      "added_tokens.json: 100%|██████████| 1.08k/1.08k [00:00<00:00, 2.84MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 99.0/99.0 [00:00<00:00, 259kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2024-01-12:22:40:42,248 WARNING  [task.py:303] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2024-01-12:22:40:42,248 WARNING  [task.py:303] has_training_docs and has_validation_docs are False, using test_docs as fewshot_docs but this is not recommended.\n",
      "2024-01-12:22:40:42,389 INFO     [task.py:358] Building contexts for task on rank 0...\n",
      "2024-01-12:22:40:52,365 INFO     [evaluator.py:256] Task: lambada_openai; number of requests on this rank: 5153\n",
      "2024-01-12:22:40:52,367 INFO     [evaluator.py:288] Running loglikelihood requests\n",
      "100%|██████████| 5153/5153 [01:07<00:00, 76.06it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bootstrapping for stddev: perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:10<00:00,  9.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hf (pretrained=microsoft/phi-2,trust_remote_code=True), limit: None, num_fewshot: None, batch_size: 32\n",
      "|    Tasks     |Version|Filter|  Metric  |Value |   |Stderr|\n",
      "|--------------|-------|------|----------|-----:|---|-----:|\n",
      "|lambada_openai|Yaml   |none  |perplexity|5.5587|±  |0.1502|\n",
      "|              |       |none  |acc       |0.6270|±  |0.0067|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source mamba_env/bin/activate\n",
    "python evals/lm_harness_eval.py --model hf \\\n",
    "    --model_args pretrained=microsoft/phi-2,trust_remote_code=True \\\n",
    "    --tasks lambada_openai --device cuda --batch_size 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Model Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2.1.2+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, tokenizer):\n",
    "    inputs = tokenizer(\"Hello, I am\", return_tensors=\"pt\")\n",
    "    tokens = model.generate(**inputs)\n",
    "    print(tokenizer.decode(tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/mambafs/mamba-hidden-states/mamba_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA driver initialization failed, you might not have a CUDA gpu.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_default_device(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/phi-2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m phi \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m test_model(phi, tokenizer)\n",
      "File \u001b[0;32m~/mambafs/mamba-hidden-states/mamba_env/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:561\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mregister(config\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, model_class, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 561\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    565\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n",
      "File \u001b[0;32m~/mambafs/mamba-hidden-states/mamba_env/lib/python3.10/site-packages/transformers/modeling_utils.py:3462\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3456\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_autoset_attn_implementation(\n\u001b[1;32m   3457\u001b[0m     config, use_flash_attention_2\u001b[38;5;241m=\u001b[39muse_flash_attention_2, torch_dtype\u001b[38;5;241m=\u001b[39mtorch_dtype, device_map\u001b[38;5;241m=\u001b[39mdevice_map\n\u001b[1;32m   3458\u001b[0m )\n\u001b[1;32m   3460\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[1;32m   3461\u001b[0m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[0;32m-> 3462\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3464\u001b[0m \u001b[38;5;66;03m# make sure we use the model's config since the __init__ call might have copied it\u001b[39;00m\n\u001b[1;32m   3465\u001b[0m config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/phi-2/eb8bbd1d37d258ea74fb082c53346d33056a83d4/modeling_phi.py:967\u001b[0m, in \u001b[0;36mPhiForCausalLM.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config):\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(config)\n\u001b[0;32m--> 967\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mPhiModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m    969\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(config\u001b[38;5;241m.\u001b[39mhidden_size, config\u001b[38;5;241m.\u001b[39mvocab_size, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/phi-2/eb8bbd1d37d258ea74fb082c53346d33056a83d4/modeling_phi.py:818\u001b[0m, in \u001b[0;36mPhiModel.__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mpad_token_id\n\u001b[1;32m    816\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[0;32m--> 818\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dropout \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(config\u001b[38;5;241m.\u001b[39membd_pdrop)\n\u001b[1;32m    820\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[1;32m    821\u001b[0m     [PhiDecoderLayer(config, layer_idx) \u001b[38;5;28;01mfor\u001b[39;00m layer_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)]\n\u001b[1;32m    822\u001b[0m )\n",
      "File \u001b[0;32m~/mambafs/mamba-hidden-states/mamba_env/lib/python3.10/site-packages/torch/nn/modules/sparse.py:142\u001b[0m, in \u001b[0;36mEmbedding.__init__\u001b[0;34m(self, num_embeddings, embedding_dim, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse, _weight, _freeze, device, dtype)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq \u001b[38;5;241m=\u001b[39m scale_grad_by_freq\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    143\u001b[0m                             requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m _freeze)\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_parameters()\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambafs/mamba-hidden-states/mamba_env/lib/python3.10/site-packages/torch/utils/_device.py:77\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambafs/mamba-hidden-states/mamba_env/lib/python3.10/site-packages/torch/cuda/__init__.py:298\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    297\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 298\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    302\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA driver initialization failed, you might not have a CUDA gpu."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "torch.set_default_device(\"cuda\")\n",
    "model_name = \"microsoft/phi-2\"\n",
    "phi = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "test_model(phi, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/mambafs/mamba-hidden-states/mamba_env/lib/python3.10/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: CUDA driver initialization failed, you might not have a CUDA gpu. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "config.json: 100%|██████████| 570/570 [00:00<00:00, 2.23MB/s]\n",
      "model.safetensors: 100%|██████████| 911M/911M [00:11<00:00, 81.2MB/s] \n",
      "tokenizer_config.json: 100%|██████████| 396/396 [00:00<00:00, 2.58MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.11M/2.11M [00:00<00:00, 25.1MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 99.0/99.0 [00:00<00:00, 685kB/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am a newbie to the world of programming. I am trying to create a program\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
    "model_name = \"EleutherAI/pythia-410m-deduped\"\n",
    "pythia = GPTNeoXForCausalLM.from_pretrained(model_name,\n",
    "                                            #revision=\"step3000\",\n",
    "                                            )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
    "  #revision=\"step3000\",\n",
    ")\n",
    "\n",
    "test_model(pythia, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "torch.set_default_device(\"cuda\")\n",
    "model_name = \"state-spaces/mamba-130m\"\n",
    "mamba = MambaLMHeadModel.from_pretrained(\n",
    "    model_name, \n",
    "    device=\"cuda\", \n",
    "    dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am a newbie to this. I have a problem with my app. I have a button that when clicked, it will open a new window. I have a button that when clicked, it will open a new window. I have a button that when clicked, it will open a new window. I have a button that when clicked, it will open a new window. I have a button that when clicked, it will open a new window. I have a button that when clicked, it will open a new window. I have a button that when clicked, it will open a new window. I have a button that when clicked, it will open a new window. I have a button that when clicked, it will open a new window. I have a button that when clicked, it will open a new window. I have a button that when clicked, it will open a new window. I have a button that when clicked, it will open a new window. I have a button that when clicked, it will open a new window. I have a button that when clicked, it will open a new window. I have a button that when clicked, it will open a new window. I have a button that when clicked, it will open a new\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"Hello, I am\", return_tensors=\"pt\")\n",
    "tokens = mamba.generate(inputs['input_ids'], max_length=256)\n",
    "print(tokenizer.decode(tokens[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune on Squadv2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mamba-hidden-states/mamba_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from training.train_mamba_with_pythia import *\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--model\", type=str, default=\"state-spaces/mamba-130m\")\n",
    "parser.add_argument(\"--output\", type=str, default=\"output\")\n",
    "parser.add_argument(\"--tokenizer\", type=str, default=\"EleutherAI/gpt-neox-20b\")\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=5e-4)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=2)\n",
    "parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=1)\n",
    "parser.add_argument(\"--optim\", type=str, default=\"adamw_torch\")\n",
    "parser.add_argument(\"--data_path\", type=str, default=\"squad\")\n",
    "parser.add_argument(\"--num_epochs\", type=int, default=10)\n",
    "args = parser.parse_args('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model = MambaLMHeadModel.from_pretrained(args.model, dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.tokenizer)\n",
    "tokenizer.eos_token = \"<|endoftext|>\"\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 0 examples, preprocess...\n",
      "Tokenizing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1000/87599 [00:00<01:07, 1288.36it/s]\n"
     ]
    }
   ],
   "source": [
    "data_module = SFTDataModule(\n",
    "    tokenizer=tokenizer,\n",
    "    data_path=args.data_path,\n",
    "    limit=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='873' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 873/5000 04:27 < 21:08, 3.25 it/s, Epoch 1.74/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>-1.081900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>-7.881900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>-10.136400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>-11.280200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>-12.048700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>-12.514700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>-13.228100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>-13.680100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>-14.391500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>-14.829700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>-15.115000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>-15.126500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>-15.746000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>-15.380600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>-16.412500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>-16.241400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>-16.573800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory output/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 22\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m MambaTrainer(\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      3\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mdata_module\u001b[38;5;241m.\u001b[39mdataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_module\u001b[38;5;241m.\u001b[39mdata_collator,\n\u001b[1;32m     20\u001b[0m )\n\u001b[0;32m---> 22\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(args\u001b[38;5;241m.\u001b[39moutput)\n",
      "File \u001b[0;32m~/mamba-hidden-states/mamba_env/lib/python3.10/site-packages/transformers/trainer.py:1537\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mamba-hidden-states/mamba_env/lib/python3.10/site-packages/transformers/trainer.py:1854\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1851\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1853\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1854\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1857\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1859\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1860\u001b[0m ):\n\u001b[1;32m   1861\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1862\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/mamba-hidden-states/mamba_env/lib/python3.10/site-packages/transformers/trainer.py:2735\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2734\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2735\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2737\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2738\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/mamba-hidden-states/training/train_mamba_with_pythia.py:108\u001b[0m, in \u001b[0;36mMambaTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m    105\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    107\u001b[0m mamba_output \u001b[38;5;241m=\u001b[39m model(input_ids, output_hidden_states\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 108\u001b[0m pythia_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpythia\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m pythia_lm_logits \u001b[38;5;241m=\u001b[39m pythia_output\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m    110\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m mamba_output\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[0;32m~/mamba-hidden-states/mamba_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mamba-hidden-states/mamba_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mamba-hidden-states/mamba_env/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:1032\u001b[0m, in \u001b[0;36mGPTNeoXForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, inputs_embeds, head_mask, past_key_values, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;124;03mpast_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;124;03m    Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;124;03m>>> prediction_logits = outputs.logits\u001b[39;00m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[1;32m   1030\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1032\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpt_neox\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1045\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1046\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_out(hidden_states)\n",
      "File \u001b[0;32m~/mamba-hidden-states/mamba_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mamba-hidden-states/mamba_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mamba-hidden-states/mamba_env/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:923\u001b[0m, in \u001b[0;36mGPTNeoXModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    912\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    913\u001b[0m         layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    914\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    920\u001b[0m         output_attentions,\n\u001b[1;32m    921\u001b[0m     )\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 923\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/mamba-hidden-states/mamba_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mamba-hidden-states/mamba_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mamba-hidden-states/mamba_env/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:688\u001b[0m, in \u001b[0;36mGPTNeoXLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, head_mask, use_cache, layer_past, output_attentions)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    680\u001b[0m     hidden_states: Optional[torch\u001b[38;5;241m.\u001b[39mFloatTensor],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    686\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    687\u001b[0m ):\n\u001b[0;32m--> 688\u001b[0m     attention_layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    697\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m attention_layer_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: attn_output, present, (attn_weights)\u001b[39;00m\n\u001b[1;32m    698\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_dropout(attn_output)\n",
      "File \u001b[0;32m~/mamba-hidden-states/mamba_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mamba-hidden-states/mamba_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mamba-hidden-states/mamba_env/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:213\u001b[0m, in \u001b[0;36mGPTNeoXAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, head_mask, layer_past, use_cache, output_attentions, padding_mask)\u001b[0m\n\u001b[1;32m    210\u001b[0m present \u001b[38;5;241m=\u001b[39m (key, value) \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# Compute attention\u001b[39;00m\n\u001b[0;32m--> 213\u001b[0m attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# Reshape outputs\u001b[39;00m\n\u001b[1;32m    216\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_heads(attn_output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_attention_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_size)\n",
      "File \u001b[0;32m~/mamba-hidden-states/mamba_env/lib/python3.10/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:289\u001b[0m, in \u001b[0;36mGPTNeoXAttention._attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# Apply the attention mask\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     attn_scores \u001b[38;5;241m=\u001b[39m attn_scores \u001b[38;5;241m+\u001b[39m attention_mask\n\u001b[0;32m--> 289\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m attn_weights\u001b[38;5;241m.\u001b[39mto(value\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    292\u001b[0m \u001b[38;5;66;03m# Mask heads if we want to\u001b[39;00m\n",
      "File \u001b[0;32m~/mamba-hidden-states/mamba_env/lib/python3.10/site-packages/torch/nn/functional.py:1856\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1854\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   1855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1856\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1858\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = MambaTrainer(\n",
    "    model=model,\n",
    "    train_dataset=data_module.dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    args=TrainingArguments(\n",
    "        learning_rate=args.learning_rate,\n",
    "        num_train_epochs=args.num_epochs,\n",
    "        per_device_train_batch_size=args.batch_size,\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "        optim=args.optim,\n",
    "        #evaluation_strategy=\"steps\",\n",
    "        #eval_steps=50,\n",
    "        output_dir=args.output,\n",
    "        save_total_limit=2,\n",
    "        logging_strategy='steps',\n",
    "        logging_steps=50,\n",
    "        save_steps=500,\n",
    "    ),\n",
    "    data_collator=data_module.data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(args.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The men's basketball team has over 1,600 wins, one of only 12 schools who have reached that mark, and have appeared in 28 NCAA tournaments. Former player Austin Carr holds the record for most points scored in a single game of the tournament with 61. Although the team has never won the NCAA Tournament, they were named by the Helms Athletic Foundation as national champions twice. The team has orchestrated a number of upsets of number one ranked teams, the most notable of which was ending UCLA's record 88-game winning streak in 1974. The team has beaten an additional eight number-one teams, and those nine wins rank second, to UCLA's 10, all-time in wins against the top team. The team plays in newly renovated Purcell Pavilion (within the Edmund P. Joyce Center), which reopened for the beginning of the 2009–2010 season. The team is coached by Mike Brey, who, as of the 2014–15 season, his fifteenth at Notre Dame, has achieved a 332-165 record. In 2009 they were invited to the NIT, where they advanced to the semifinals but were beaten by Penn State who went on and beat Baylor in the championship. The 2010–11 team concluded its regular season ranked number seven in the country, with a record of 25–5, Brey's fifth straight 20-win season, and a second-place finish in the Big East. During the 2014-15 season, the team went 32-6 and won the ACC conference tournament, later advancing to the Elite 8, where the Fighting Irish lost on a missed buzzer-beater against then undefeated Kentucky. Led by NBA draft picks Jerian Grant and Pat Connaughton, the Fighting Irish beat the eventual national champion Duke Blue Devils twice during the season. The 32 wins were the most by the Fighting Irish team since 1908-09.\n",
      "Q: How many schools have a similar men's basketball record to Notre Dame in terms of wins?\n",
      "\n",
      "The men's basketball team has over 1,600 wins, one of only 12 schools who have reached that mark, and have appeared in 28 NCAA tournaments. Former player Austin Carr holds the record for most points scored in a single game of the tournament with 61. Although the team has never won the NCAA Tournament, they were named by the Helms Athletic Foundation as national champions twice. The team has orchestrated a number of upsets of number one ranked teams, the most notable of which was ending UCLA's record 88-game winning streak in 1974. The team has beaten an additional eight number-one teams, and those nine wins rank second, to UCLA's 10, all-time in wins against the top team. The team plays in newly renovated Purcell Pavilion (within the Edmund P. Joyce Center), which reopened for the beginning of the 2009–2010 season. The team is coached by Mike Brey, who, as of the 2014–15 season, his fifteenth at Notre Dame, has achieved a 332-165 record. In 2009 they were invited to the NIT, where they advanced to the semifinals but were beaten by Penn State who went on and beat Baylor in the championship. The 2010–11 team concluded its regular season ranked number seven in the country, with a record of 25–5, Brey's fifth straight 20-win season, and a second-place finish in the Big East. During the 2014-15 season, the team went 32-6 and won the ACC conference tournament, later advancing to the Elite 8, where the Fighting Irish lost on a missed buzzer-beater against then undefeated Kentucky. Led by NBA draft picks Jerian Grant and Pat Connaughton, the Fighting Irish beat the eventual national champion Duke Blue Devils twice during the season. The 32 wins were the most by the Fighting Irish team since 1908-09.\n",
      "Q: How many schools have a similar men's basketball record to Notre Dame in terms of wins?\n",
      "A\n",
      "\n",
      "A: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mamba-hidden-states/mamba_env/lib/python3.10/site-packages/transformers/generation/utils.py:1363: UserWarning: Input length of input_ids is 407, but `max_length` is set to 256. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "inputs = data_module.dataset[5]\n",
    "prompt, answer = tokenizer.decode(inputs['input_ids']).split('A:')\n",
    "answer = 'A:'+answer\n",
    "print(prompt)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "tokens = trainer.pythia.generate(inputs['input_ids'].to(torch.device('cuda:0')), max_length=256)\n",
    "print(tokenizer.decode(tokens[0]))\n",
    "print()\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beyoncé embarked on The Mrs. Carter Show World Tour on April 15 in Belgrade, Serbia; the tour included 132 dates that ran through to March 2014. It became the most successful tour of her career and one of the most-successful tours of all time. In May, Beyoncé's cover of Amy Winehouse's \"Back to Black\" with André 3000 on The Great Gatsby soundtrack was released. She was also honorary chair of the 2013 Met Gala. Beyoncé voiced Queen Tara in the 3D CGI animated film, Epic, released by 20th Century Fox on May 24, and recorded an original song for the film, \"Rise Up\", co-written with Sia.\n",
      "Q: One of Beyonce's most successful tours yet was which one?\n",
      "\n",
      "Beyoncé embarked on The Mrs. Carter Show World Tour on April 15 in Belgrade, Serbia; the tour included 132 dates that ran through to March 2014. It became the most successful tour of her career and one of the most-successful tours of all time. In May, Beyoncé's cover of Amy Winehouse's \"Back to Black\" with André 3000 on The Great Gatsby soundtrack was released. She was also honorary chair of the 2013 Met Gala. Beyoncé voiced Queen Tara in the 3D CGI animated film, Epic, released by 20th Century Fox on May 24, and recorded an original song for the film, \"Rise Up\", co-written with Sia.\n",
      "Q: One of Beyonce's most successful tours yet was which one?\n",
      "A: 4<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "\n",
      "A: The Mrs. Carter Show\n"
     ]
    }
   ],
   "source": [
    "inputs = data_module.dataset[5]\n",
    "prompt, answer = tokenizer.decode(inputs['input_ids']).split('A:')\n",
    "answer = 'A:'+answer\n",
    "print(prompt)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "tokens = trainer.model.generate(inputs['input_ids'].to(torch.device('cuda:0')), max_length=256)\n",
    "print(tokenizer.decode(tokens[0]))\n",
    "print()\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 46.00 MiB. GPU 0 has a total capacty of 15.74 GiB of which 16.62 MiB is free. Process 3736616 has 2.19 GiB memory in use. Process 3790542 has 13.53 GiB memory in use. Of the allocated memory 12.41 GiB is allocated by PyTorch, and 993.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m inputs \u001b[38;5;241m=\u001b[39m data_module\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokenizer\u001b[38;5;241m.\u001b[39mdecode(tokens[\u001b[38;5;241m0\u001b[39m]))\n",
      "File \u001b[0;32m~/mamba-hidden-states/mamba_ssm/utils/generation.py:244\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, input_ids, max_length, top_k, top_p, temperature, return_dict_in_generate, output_scores, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    235\u001b[0m     input_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    243\u001b[0m ):\n\u001b[0;32m--> 244\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_scores:\n\u001b[1;32m    248\u001b[0m         output\u001b[38;5;241m.\u001b[39mscores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mamba-hidden-states/mamba_env/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mamba-hidden-states/mamba_ssm/utils/generation.py:206\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m(input_ids, model, max_length, top_k, top_p, temperature, repetition_penalty, eos_token_id, teacher_outputs, vocab_size, cg, enable_timing, streamer)\u001b[0m\n\u001b[1;32m    204\u001b[0m sequences_cat \u001b[38;5;241m=\u001b[39m input_ids\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_stop(sequences[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], inference_params):\n\u001b[0;32m--> 206\u001b[0m     scores\u001b[38;5;241m.\u001b[39mappend(\u001b[43mget_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minference_params\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    207\u001b[0m     inference_params\u001b[38;5;241m.\u001b[39mseqlen_offset \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m sequences[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m repetition_penalty \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n",
      "File \u001b[0;32m~/mamba-hidden-states/mamba_ssm/utils/generation.py:169\u001b[0m, in \u001b[0;36mdecode.<locals>.get_logits\u001b[0;34m(input_ids, inference_params)\u001b[0m\n\u001b[1;32m    167\u001b[0m     position_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cg \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m decoding:\n\u001b[0;32m--> 169\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43minference_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minference_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_last_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39msqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_decoding_cache\u001b[38;5;241m.\u001b[39mrun(\n\u001b[1;32m    177\u001b[0m         input_ids, position_ids, inference_params\u001b[38;5;241m.\u001b[39mseqlen_offset\n\u001b[1;32m    178\u001b[0m     )\u001b[38;5;241m.\u001b[39msqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/mamba-hidden-states/mamba_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mamba-hidden-states/mamba_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mamba-hidden-states/mamba_ssm/models/mixer_seq_simple.py:253\u001b[0m, in \u001b[0;36mMambaLMHeadModel.forward\u001b[0;34m(self, input_ids, position_ids, inference_params, num_last_tokens, output_hidden_states)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_last_tokens \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    252\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states[:, \u001b[38;5;241m-\u001b[39mnum_last_tokens:]\n\u001b[0;32m--> 253\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CausalLMOutput(\n\u001b[1;32m    255\u001b[0m     logits\u001b[38;5;241m=\u001b[39mlm_logits,\n\u001b[1;32m    256\u001b[0m     hidden_states\u001b[38;5;241m=\u001b[39moutput\u001b[38;5;241m.\u001b[39mhidden_states\n\u001b[1;32m    257\u001b[0m )\n",
      "File \u001b[0;32m~/mamba-hidden-states/mamba_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mamba-hidden-states/mamba_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mamba-hidden-states/mamba_env/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 46.00 MiB. GPU 0 has a total capacty of 15.74 GiB of which 16.62 MiB is free. Process 3736616 has 2.19 GiB memory in use. Process 3790542 has 13.53 GiB memory in use. Of the allocated memory 12.41 GiB is allocated by PyTorch, and 993.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "inputs = data_module.dataset[0]\n",
    "tokens = trainer.model.generate(inputs['input_ids'].unsqueeze(-1).cuda(), max_length=256)\n",
    "print(tokenizer.decode(tokens[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on Squadv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python evals/lm_harness_eval.py --model mamba \\\n",
    "    --model_args pretrained=state-spaces/mamba-130m \\\n",
    "    --tasks squadv2 --device cuda --batch_size 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, GPTNeoXForCausalLM, GPTNeoXModel\n",
    "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
    "import sys\n",
    "torch.set_default_device(\"cuda\")\n",
    "\n",
    "\n",
    "# Take in the model you want to train\n",
    "model_name = \"state-spaces/mamba-130m\"\n",
    "\n",
    "# Choose a tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "tokenizer.eos_token = \"<|endoftext|>\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "tokenizer_pythia = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-160m-deduped\")\n",
    "tokenizer_pythia.eos_token = \"<|endoftext|>\"\n",
    "tokenizer_pythia.pad_token = tokenizer_pythia.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer, GPTNeoXModel\n",
    "\n",
    "pythia = GPTNeoXForCausalLM.from_pretrained(\n",
    "  \"EleutherAI/pythia-410m-deduped\",\n",
    "  #output_hidden_states=True,\n",
    "  #revision=\"step3000\",\n",
    "  #cache_dir=\"./pythia-70m-deduped/step3000\",\n",
    ").to(torch.device('cuda:0'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mamba = MambaLMHeadModel.from_pretrained(\n",
    "    model_name, \n",
    "    device=\"cuda\", \n",
    "    dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the user input from the command line\n",
    "user_message = \"Give me three steps to improve my diet, and include some evidence\"#input(\"\\n> \")\n",
    "\n",
    "# Create a prompt\n",
    "n_shot_prompting = [\n",
    "    {\n",
    "        \"question\": \"What is the capital of France?\",\n",
    "        \"answer\": \"Paris\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Who invented the segway?\",\n",
    "        \"answer\": \"Dean Kamen\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the fastest animal?\",\n",
    "        \"answer\": \"Cheetah\"\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt = f\"You are a Trivia QA bot.\\nAnswer the following question succinctly and accurately.\"\n",
    "prompt = f\"{prompt}\\n\\n\" + \"\\n\\n\".join([f\"Q: {p['question']}\\nA: {p['answer']}\" for p in n_shot_prompting])\n",
    "prompt = f\"{prompt}\\n\\nQ: {user_message}\\nA:\"\n",
    "\n",
    "# Debug print to make sure our prompt looks good\n",
    "print(prompt)\n",
    "\n",
    "# Encode the text to token IDs\n",
    "input_ids = torch.LongTensor([tokenizer.encode(prompt)]).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an output sequence of tokens given the input\n",
    "# \"out\" will contain the raw token ids as integers\n",
    "out = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=256,\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pythia_out = pythia(\n",
    "    input_ids=input_ids,\n",
    "    output_hidden_states=True\n",
    ")\n",
    "\n",
    "mamba_out = mamba(\n",
    "    input_ids=input_ids,\n",
    "    output_hidden_states=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_loss = (pythia_out.logits.softmax(dim=2)[:,:,:50280] - mamba_out.logits.softmax(dim=2)).norm(dim=2).mean()\n",
    "teacher_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import math\n",
    "mu = 0\n",
    "std = math.sqrt(1.0/mamba_out.hidden_states[0].shape[-1])\n",
    "size = (1, pythia_out.hidden_states[0].shape[-1], mamba_out.hidden_states[0].shape[-1])\n",
    "W = torch.normal(0, std, size).to(torch.device('cuda:0'))\n",
    "\n",
    "\n",
    "F.cosine_similarity(pythia_out.hidden_states[0]@ W,  mamba_out.hidden_states[0], dim=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,X in enumerate(pythia_out.hidden_states):\n",
    "    print(k, X.shape)\n",
    "for k,X in enumerate(mamba_out.hidden_states):\n",
    "    print(k, X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you must use the tokenizer to decode them back into strings\n",
    "decoded = tokenizer.batch_decode(out)[0]\n",
    "print(\"=\"*80)\n",
    "print(decoded)\n",
    "# out returns the whole sequence plus the original\n",
    "cleaned = decoded.replace(prompt, \"\")\n",
    "\n",
    "# the model will just keep generating, so only grab the first one\n",
    "# cleaned = cleaned.split(\"\\n\\n\")[0]\n",
    "print(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/test-fs2/project/mamba-hidden-states/mambaphi/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 0 examples, preprocess...\n",
      "Tokenizing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87599/87599 [01:57<00:00, 746.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3749' max='438000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  3749/438000 22:00 < 42:30:00, 2.84 it/s, Epoch 0.09/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.279300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.242800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.301800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.264300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.227300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.267300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.249800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.214700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.216000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.265800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.219800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.222200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.211600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.169500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.235200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.158800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.202700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.189000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.189700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.210300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.232300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.218900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.202700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.331400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.310400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.313400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.296900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.328900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.292700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.359700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.336400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.294500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.277700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.310000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.310800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.278900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.290400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.260200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.289600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.304800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.279800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.326600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.271000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.277600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.259100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.277300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.298700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.304300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.257400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.291700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.296800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.253600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.286400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.287000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.272900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.278300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.242000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.264500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.305200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.269400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.273300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.262300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>0.298300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.278200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>0.255000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.260800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>0.279800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.261500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>0.255100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.273500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>0.250900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.271300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>0.261200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.285400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 32\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m      9\u001b[0m data_module \u001b[38;5;241m=\u001b[39m SFTDataModule(\n\u001b[1;32m     10\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     11\u001b[0m     data_path\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mdata_path,\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m trainer \u001b[38;5;241m=\u001b[39m MambaTrainer(\n\u001b[1;32m     15\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     16\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mdata_module\u001b[38;5;241m.\u001b[39mdataset,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_module\u001b[38;5;241m.\u001b[39mdata_collator,\n\u001b[1;32m     30\u001b[0m )\n\u001b[0;32m---> 32\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(args\u001b[38;5;241m.\u001b[39moutput)\n",
      "File \u001b[0;32m~/test-fs2/project/mamba-hidden-states/mambaphi/lib/python3.10/site-packages/transformers/trainer.py:1537\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1535\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1537\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/test-fs2/project/mamba-hidden-states/mambaphi/lib/python3.10/site-packages/transformers/trainer.py:1859\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1853\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   1854\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1857\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m-> 1859\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misinf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss_step\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1860\u001b[0m ):\n\u001b[1;32m   1861\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1862\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   1863\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba_env",
   "language": "python",
   "name": "mamba_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
