{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa181da5",
   "metadata": {},
   "source": [
    "More details about each task can be found in  the documentation in [`docs/README.md`](https://github.com/bigcode-project/bigcode-evaluation-harness/blob/main/docs/README.md).\n",
    "## Setup\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/bigcode-project/bigcode-evaluation-harness.git\n",
    "cd bigcode-evaluation-harness\n",
    "```\n",
    "Install [`torch`](https://pytorch.org/get-started/locally/) based on your device type, and install the other packages using:\n",
    "```\n",
    "pip install -e .\n",
    "```\n",
    "To run the `DS-1000` benchmark, additional constraints must be resolved.\n",
    "```\n",
    "# python version must be 3.7.10\n",
    "pip install -e \".[ds1000]\" # installs all additional dependencies except PyTorch\n",
    "# torch==1.12.1 required. Download version with relevant GPU support etc., e.g.,\n",
    "pip install torch==1.12.1+cu116 --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "\n",
    "# to suppress any tensorflow optimization warnings, \n",
    "# precede call to \"accelerate launch\" with \"TF_CPP_MIN_LOG_LEVEL=3\"\n",
    "\n",
    "# on some systems, tensorflow will attempt to allocate all GPU memory\n",
    "# to its process at import which will raise a CUDA out-of-memory error\n",
    "# setting \"export TF_FORCE_GPU_ALLOW_GROWTH=true\" resolves this\n",
    "```\n",
    "Also make sure you have `git-lfs` installed and are logged in the Hub\n",
    "```\n",
    "huggingface-cli login\n",
    "````\n",
    "\n",
    "We use [`accelerate`](https://huggingface.co/docs/accelerate/index) to generate code/text in parallel when multiple GPUs are present (multi-GPU mode). You can configure it using:\n",
    "\n",
    "```bash\n",
    "accelerate config\n",
    "```\n",
    "\n",
    "This evaluation harness can also be used in an evaluation only mode, you can use a Multi-CPU setting. For large models, we recommend specifying the precision of the model using the `--precision` flag instead of accelerate config to have only one copy of the model in memory. You can also load models in 8bit with the flag `--load_in_8bit` or 4bit with `--load_in_4bit` if you have `bitsandbytes` installed with the required transformers and accelerate versions.\n",
    "\n",
    "The evaluation part (solutions execution) for [MultiPL-E](https://github.com/nuprl/MultiPL-E) requires extra dependencies for some programming languages, we provide a Dockerfile with all dependencies, see section [Docker](#docker-containers) for more details.\n",
    "\n",
    "## Usage\n",
    "You can use this evaluation harness to generate text solutions to code benchmarks with your model, to evaluate (and execute) the solutions or to do both. While it is better to use GPUs for the generation, the evaluation only requires CPUs. So it might be beneficial to separate these two steps. By default both generation and evaluation are performed.\n",
    "\n",
    "For more details on how to evaluate on the tasks, please refer to the documentation in [`docs/README.md`](https://github.com/bigcode-project/bigcode-evaluation-harness/blob/main/docs/README.md). \n",
    "\n",
    "### Generation and evaluation\n",
    "Below is an example to generate and evaluate on a task.\n",
    "\n",
    "```bash\n",
    "accelerate launch  main.py \\\n",
    "  --model <MODEL_NAME> \\\n",
    "  --tasks <TASK_NAME> \\\n",
    "  --limit <NUMBER_PROBLEMS> \\\n",
    "  --max_length_generation <MAX_LENGTH> \\\n",
    "  --temperature <TEMPERATURE> \\\n",
    "  --do_sample True \\\n",
    "  --n_samples 100 \\\n",
    "  --batch_size 10 \\\n",
    "  --precision <PRECISION> \\\n",
    "  --allow_code_execution \\\n",
    "  --save_generations\n",
    "```\n",
    "* `limit` represents the number of problems to solve, if it's not provided all problems in the benchmark are selected. \n",
    "* `allow_code_execution` is for executing the generated code: it is off by default, read the displayed warning before calling it to enable execution. \n",
    "* Some models with custom code on the HF hub like [SantaCoder](https://huggingface.co/bigcode/santacoder) require calling `--trust_remote_code`, for private models add `--use_auth_token`.\n",
    "* `save_generations` saves the post-processed generations in a json file at `save_generations_path` (by default `generations.json`). You can also save references by calling `--save_references`\n",
    "* `max_length_generation` is the maximum token length of generation including the input token length. The default is 512, but for some tasks like GSM8K and GSM-Hard, the complete prompt with 8 shot examples (as used in [PAL](https://github.com/reasoning-machines/pal)) take up `~1500` tokens, hence the value should be greater than that and the recommended value of `max_length_generation` is `2048` for these tasks.\n",
    "\n",
    "Some tasks don't require code execution such as\n",
    "`codexglue_code_to_text-<LANGUAGE>`/`codexglue_code_to_text-python-left`/`conala`/`concode` that use BLEU evaluation. In addition, we generate one candidate solution for each problem in these tasks, so use `n_samples=1` and `batch_size=1`. (Note that `batch_size` should always be equal or less than `n_samples`).\n",
    "* For APPS tasks, you can use `n_samples=1` for strict and average accuracies (from the original APPS paper) and `n_samples>1` for pass@k.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ccea0d-fa06-40b4-8f07-2042693a8e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerate launch  main.py \\\n",
    "  --model \"microsoft/phi-2\" \\\n",
    "  --max_length_generation 528 \\\n",
    "  --save_generations \\\n",
    "  --tasks humaneval \\\n",
    "  --n_samples 1 \\\n",
    "  --batch_size 1 \\\n",
    "  --allow_code_execution \\\n",
    "  --trust_remote_code \\\n",
    "  --limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ad9502",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerate launch  main.py \\\n",
    "  --model \"microsoft/phi-2\" \\\n",
    "  --max_length_generation 528 \\\n",
    "  --save_generations \\\n",
    "  --tasks humaneval \\\n",
    "  --n_samples 1 \\\n",
    "  --batch_size 1 \\\n",
    "  --allow_code_execution \\\n",
    "  --trust_remote_code \\\n",
    "  --limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2e813f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35e37ef7",
   "metadata": {},
   "source": [
    "## Evaluations\n",
    "\n",
    "To run zero-shot evaluations of models (corresponding to Table 3 of the paper),\n",
    "we use the\n",
    "[lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness/tree/big-refactor)\n",
    "library.\n",
    "\n",
    "1. Pull the `lm-evaluation-harness` repo by `git submodule update --init\n",
    "   --recursive`. We use the `big-refactor` branch.\n",
    "2. Install `lm-evaluation-harness`: `pip install -e 3rdparty/lm-evaluation-harness`.\n",
    "On Python 3.10 you might need to manually install the latest version of `promptsource`: `pip install git+https://github.com/bigscience-workshop/promptsource.git`.\n",
    "3. Run evaluation with (more documentation at the [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness/tree/big-refactor) repo):\n",
    "```\n",
    "python evals/lm_harness_eval.py --model mamba --model_args pretrained=state-spaces/mamba-130m --tasks lambada_openai,hellaswag,piqa,arc_easy,arc_challenge,winogrande --device cuda --batch_size 64\n",
    "python evals/lm_harness_eval.py --model hf --model_args pretrained=EleutherAI/pythia-160m --tasks lambada_openai,hellaswag,piqa,arc_easy,arc_challenge,winogrande --device cuda --batch_size 64\n",
    "```\n",
    "\n",
    "To reproduce the results on the `mamba-2.8b-slimpj` model reported in the blogposts:\n",
    "```\n",
    "python evals/lm_harness_eval.py --model mamba --model_args pretrained=state-spaces/mamba-2.8b-slimpj --tasks boolq,piqa,hellaswag,winogrande,arc_easy,arc_challenge,openbookqa,race,truthfulqa_mc2 --device cuda --batch_size 64\n",
    "python evals/lm_harness_eval.py --model mamba --model_args pretrained=state-spaces/mamba-2.8b-slimpj --tasks mmlu --num_fewshot 5 --device cuda --batch_size 64\n",
    "```\n",
    "\n",
    "Note that the result of each task might differ from reported values by 0.1-0.3 due to noise in the evaluation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e53c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_eval --model hf \\\n",
    "    --model_args pretrained=EleutherAI/pythia-410m,revision=step100000,dtype=\"float\" \\\n",
    "    --tasks squadv2 \\       \n",
    "--device cpu \\\n",
    "--batch_size 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
